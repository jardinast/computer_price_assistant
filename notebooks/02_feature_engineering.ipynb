{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 02 - Feature Engineering: Computer Price Prediction\n",
    "\n",
    "## Simplified and Improved Pipeline\n",
    "\n",
    "This notebook applies **simplified feature engineering** with:\n",
    "\n",
    "1. **Data Quality Analysis** - Detect format issues, mixed types, and column groupings\n",
    "2. **CPU/GPU Parsing** - Extract normalized keys (brand, family, model, suffix)\n",
    "3. **Benchmark Matching** - Exact matching first, then fuzzy matching with scores\n",
    "4. **Feature Extraction** - 18 engineered features for modeling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nimport warnings\n\n# Reload features module\nsys.path.append('..')\nfor mod in ['src.features', 'features']:\n    if mod in sys.modules:\n        del sys.modules[mod]\n\nfrom src.features import (\n    cargar_datos, construir_features,\n    analyze_format_issues, print_column_groups,\n    parse_cpu_name, parse_gpu_name,\n    prepare_modeling_data, get_feature_columns\n)\n\n# Display settings\npd.set_option('display.max_columns', 50)\npd.set_option('display.max_rows', 100)\npd.set_option('display.float_format', '{:.2f}'.format)\nsns.set_theme(style='whitegrid')\nwarnings.filterwarnings('ignore')\n\nprint(\"Libraries loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../data')\n",
    "\n",
    "df_comp, df_cpu, df_gpu = cargar_datos(\n",
    "    str(DATA_DIR / 'db_computers_2025_raw.csv'),\n",
    "    str(DATA_DIR / 'db_cpu_raw.csv'),\n",
    "    str(DATA_DIR / 'db_gpu_raw.csv')\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset shapes:\")\n",
    "print(f\"  Computers: {df_comp.shape}\")\n",
    "print(f\"  CPU benchmarks: {df_cpu.shape}\")\n",
    "print(f\"  GPU benchmarks: {df_gpu.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Data Quality Analysis\n",
    "\n",
    "Detect format issues, mixed types, and column groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze format issues in each dataset\n",
    "df_comp_issues = analyze_format_issues(df_comp, \"Computers dataset\")\n",
    "df_cpu_issues = analyze_format_issues(df_cpu, \"CPU benchmarks\")\n",
    "df_gpu_issues = analyze_format_issues(df_gpu, \"GPU benchmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show columns with format issues\n",
    "print(\"\\n=== Computers: Columns with mixed types ===\")\n",
    "mixed_cols = df_comp_issues[df_comp_issues['mixed_numeric_text'] == True]\n",
    "if len(mixed_cols) > 0:\n",
    "    display(mixed_cols[['column', 'sample_text_values']].head(10))\n",
    "else:\n",
    "    print(\"No mixed type columns detected\")\n",
    "\n",
    "print(\"\\n=== Computers: Columns with multilabel values ===\")\n",
    "multilabel = df_comp_issues[df_comp_issues['multilabel_rows'] > 100].sort_values('multilabel_rows', ascending=False)\n",
    "if len(multilabel) > 0:\n",
    "    display(multilabel[['column', 'multilabel_rows']].head(10))\n",
    "else:\n",
    "    print(\"No significant multilabel columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show column groups by prefix\n",
    "print_column_groups(df_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. CPU/GPU Parsing Preview\n",
    "\n",
    "Test the parsing logic on sample data before running full feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CPU parsing on sample data\n",
    "print(\"=== CPU Parsing Examples ===\")\n",
    "cpu_samples = df_comp['Procesador_Procesador'].dropna().sample(10, random_state=42)\n",
    "\n",
    "for cpu in cpu_samples:\n",
    "    parsed = parse_cpu_name(cpu)\n",
    "    print(f\"\\nOriginal: {cpu}\")\n",
    "    print(f\"  -> Key: {parsed['cpu_normalized_key']}\")\n",
    "    print(f\"  -> Brand: {parsed['cpu_brand']}, Family: {parsed['cpu_family']}\")\n",
    "    print(f\"  -> Model: {parsed['cpu_model_code']}, Suffix: {parsed['cpu_suffix']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GPU parsing on sample data\n",
    "print(\"=== GPU Parsing Examples ===\")\n",
    "gpu_samples = df_comp['Gráfica_Tarjeta gráfica'].dropna().sample(10, random_state=21)\n",
    "\n",
    "for gpu in gpu_samples:\n",
    "    parsed = parse_gpu_name(gpu)\n",
    "    print(f\"\\nOriginal: {gpu}\")\n",
    "    print(f\"  -> Key: {parsed['gpu_normalized_key']}\")\n",
    "    print(f\"  -> Brand: {parsed['gpu_brand']}, Series: {parsed['gpu_series']}\")\n",
    "    print(f\"  -> Model: {parsed['gpu_model_number']}, Integrated: {parsed['gpu_is_integrated']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Run Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build all engineered features\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nProcessing {len(df_comp):,} computer listings...\\n\")\n",
    "\n",
    "df_feat = construir_features(df_comp, df_cpu, df_gpu)\n",
    "\n",
    "print(f\"\\nDataframe shape: {df_feat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all engineered features\n",
    "engineered = sorted([c for c in df_feat.columns if c.startswith('_')])\n",
    "print(f\"\\nTotal engineered features: {len(engineered)}\\n\")\n",
    "\n",
    "for i, feat in enumerate(engineered, 1):\n",
    "    non_null = df_feat[feat].notna().sum()\n",
    "    pct = non_null / len(df_feat) * 100\n",
    "    print(f\"{i:2d}. {feat:35s}: {non_null:5,}/{len(df_feat):,} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. CPU/GPU Matching Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU matching summary\n",
    "print(\"=\" * 60)\n",
    "print(\"CPU BENCHMARK MATCHING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nMatch strategy distribution:\")\n",
    "print(df_feat['cpu_match_strategy'].value_counts(dropna=False))\n",
    "\n",
    "# Coverage by brand\n",
    "print(\"\\nCoverage by CPU brand:\")\n",
    "for brand in ['intel', 'amd', 'apple', 'qualcomm']:\n",
    "    mask = df_feat['cpu_brand'] == brand\n",
    "    if mask.sum() > 0:\n",
    "        matched = df_feat.loc[mask, 'cpu_bench_mark'].notna().sum()\n",
    "        total = mask.sum()\n",
    "        print(f\"  {brand.capitalize():10s}: {matched:4,}/{total:4,} ({matched/total*100:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU matching summary\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU BENCHMARK MATCHING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nMatch strategy distribution:\")\n",
    "print(df_feat['gpu_match_strategy'].value_counts(dropna=False))\n",
    "\n",
    "# Show discrete GPU coverage\n",
    "discrete_mask = df_feat['gpu_is_integrated'] != True\n",
    "if discrete_mask.sum() > 0:\n",
    "    matched = df_feat.loc[discrete_mask, 'gpu_bench_mark'].notna().sum()\n",
    "    total = discrete_mask.sum()\n",
    "    print(f\"\\nDiscrete GPU coverage: {matched:,}/{total:,} ({matched/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample matched CPUs\n",
    "print(\"\\n=== Sample Matched CPUs ===\")\n",
    "matched_cpus = df_feat[df_feat['cpu_match_strategy'].isin(['exact', 'fuzzy'])][\n",
    "    ['Procesador_Procesador', 'cpu_normalized_key', 'cpu_bench_name', \n",
    "     'cpu_bench_mark', 'cpu_match_strategy', 'cpu_match_score']\n",
    "].head(15)\n",
    "display(matched_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample matched GPUs\n",
    "print(\"\\n=== Sample Matched GPUs ===\")\n",
    "matched_gpus = df_feat[df_feat['gpu_match_strategy'].isin(['exact', 'fuzzy'])][\n",
    "    ['Gráfica_Tarjeta gráfica', 'gpu_normalized_key', 'gpu_bench_name', \n",
    "     'gpu_bench_mark', 'gpu_match_strategy', 'gpu_match_score']\n",
    "].head(15)\n",
    "display(matched_gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# Correlation with price\nnumeric_feats = [\n    '_ram_gb', '_ssd_gb', '_cpu_cores', '_gpu_memory_gb',\n    # CPU benchmark features\n    '_cpu_mark', '_cpu_rank', '_cpu_value', '_cpu_price_usd',\n    # GPU benchmark features\n    '_gpu_mark', '_gpu_rank', '_gpu_value', '_gpu_price_usd',\n    # Other features\n    '_tamano_pantalla_pulgadas', '_resolucion_pixeles', \n    '_tasa_refresco_hz', '_peso_kg', '_precio_num'\n]\n\navailable = [f for f in numeric_feats if f in df_feat.columns]\ncorr = df_feat[available].corr()['_precio_num'].drop('_precio_num').sort_values(ascending=False)\n\nprint(\"=\" * 60)\nprint(\"CORRELATION WITH PRICE\")\nprint(\"=\" * 60)\nprint(f\"\\n{'Feature':<35s} {'Correlation':>12s} {'Strength':>12s}\")\nprint(\"-\" * 60)\n\nfor feat, c in corr.items():\n    if pd.notna(c):\n        strength = \"Strong\" if abs(c) >= 0.5 else \"Moderate\" if abs(c) >= 0.3 else \"Weak\"\n        print(f\"{feat:<35s} {c:>12.3f} {strength:>12s}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "corr.plot(kind='barh', ax=ax, color=['green' if x > 0 else 'red' for x in corr])\n",
    "ax.set_xlabel('Correlation with Price')\n",
    "ax.set_title('Feature Correlations with Price')\n",
    "ax.axvline(0.3, color='orange', linestyle='--', alpha=0.5)\n",
    "ax.axvline(0.5, color='red', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 8. Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "# Plot distributions of key features (including new benchmark features)\nkey_feats = [\n    '_precio_num', '_ram_gb', '_ssd_gb', '_cpu_cores', \n    '_cpu_mark', '_cpu_rank', '_cpu_value',\n    '_gpu_mark', '_gpu_rank', '_gpu_value'\n]\nkey_feats = [f for f in key_feats if f in df_feat.columns]\n\nn_cols = 3\nn_rows = (len(key_feats) + n_cols - 1) // n_cols\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\naxes = axes.ravel()\n\nfor idx, feat in enumerate(key_feats):\n    if idx < len(axes):\n        data = df_feat[feat].dropna()\n        if len(data) > 0:\n            axes[idx].hist(data, bins=50, edgecolor='black', alpha=0.7)\n            axes[idx].set_xlabel(feat)\n            axes[idx].set_title(f'Distribution of {feat}')\n            axes[idx].axvline(data.median(), color='red', linestyle='--', \n                             label=f'Median: {data.median():.1f}')\n            axes[idx].legend()\n\n# Hide unused subplots\nfor idx in range(len(key_feats), len(axes)):\n    axes[idx].set_visible(False)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 9. Missing Values Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values for engineered features\n",
    "eng_feats = [c for c in df_feat.columns if c.startswith('_')]\n",
    "missing = pd.DataFrame({\n",
    "    'Missing': df_feat[eng_feats].isna().sum(),\n",
    "    'Missing %': (df_feat[eng_feats].isna().sum() / len(df_feat) * 100).round(1)\n",
    "}).sort_values('Missing %', ascending=False)\n",
    "\n",
    "print(\"=== Missing Values for Engineered Features ===\")\n",
    "display(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 10. Prepare Data for Modeling\n\nFilter features with >60% missing values and prepare data for the modeling notebook."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": "# Prepare data for modeling (removes features with >60% missing values)\ndf_model = prepare_modeling_data(df_feat, target_col='_precio_num', max_missing_pct=0.60)\n\n# Get feature column lists\nall_features, numeric_features, categorical_features = get_feature_columns(df_model)\n\nprint(f\"\\nFeature summary for modeling:\")\nprint(f\"  Total features: {len(all_features)}\")\nprint(f\"  Numeric features: {len(numeric_features)}\")\nprint(f\"  Categorical features: {len(categorical_features)}\")\n\n# Save processed dataset - try parquet first, fall back to CSV\ntry:\n    output_path = DATA_DIR / 'db_features.parquet'\n    df_model.to_parquet(output_path, index=False)\n    print(f\"\\nSaved to: {output_path}\")\n    print(f\"File size: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\nexcept ImportError:\n    print(\"\\nNote: pyarrow not installed, saving as CSV instead\")\n    output_path = DATA_DIR / 'db_features.csv'\n    df_model.to_csv(output_path, index=False)\n    print(f\"\\nSaved to: {output_path}\")\n    print(f\"File size: {output_path.stat().st_size / 1024 / 1024:.2f} MB\")\n\nprint(f\"Rows: {len(df_model):,}\")\nprint(f\"Columns: {len(df_model.columns)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": "## Summary\n\n### Feature Engineering Complete!\n\n**Key Improvements:**\n\n1. **Simplified CPU/GPU Parsing** - Extract normalized keys with brand, family, model, suffix\n2. **Multi-Stage Matching** - Exact match → fuzzy match → close neighbor → family/brand mean\n3. **Match Tracking** - `cpu_match_strategy` and `gpu_match_strategy` columns track how each row was matched\n4. **Integrated GPU Handling** - Correctly identifies and skips integrated graphics\n5. **Complete Benchmark Features** - All 4 benchmark data points for CPU and GPU:\n   - `_cpu_mark`, `_cpu_rank`, `_cpu_value`, `_cpu_price_usd`\n   - `_gpu_mark`, `_gpu_rank`, `_gpu_value`, `_gpu_price_usd`\n6. **Missing Value Filtering** - Removed features with >60% missing values\n\n**Columns Excluded from Features:**\n- `cpu_match_score`, `gpu_match_score` - Intermediate matching metadata\n- `Ofertas`, `Num_ofertas` - Dataset-specific, not useful for prediction\n\n**Next Steps:**\n- Load processed dataset in modeling notebook\n- Train and evaluate ML models with clean data"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}