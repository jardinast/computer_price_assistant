{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Model Optimization: Feature Selection, Tuning & Ensemble\n",
    "\n",
    "This notebook optimizes the price prediction model through:\n",
    "\n",
    "1. **Feature Importance Analysis** - Identify and remove unimportant features\n",
    "2. **User Input Considerations** - Which features are easy for users to provide?\n",
    "3. **Additional Models** - XGBoost, LightGBM\n",
    "4. **Hyperparameter Tuning** - Optuna optimization for top models\n",
    "5. **Ensemble Methods** - Stacking and weighted averaging\n",
    "6. **Final Model Selection** - Best single model vs ensemble\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nimport warnings\n\n# Add src to path\nsys.path.append('..')\n\n# Reload modules\nfor mod in list(sys.modules.keys()):\n    if 'src.' in mod or mod in ['modeling', 'features']:\n        del sys.modules[mod]\n\nfrom src.modeling import (\n    build_sklearn_pipeline,\n    build_catboost_model,\n    prepare_catboost_data,\n    evaluate_sklearn_cv,\n    evaluate_predictions,\n    compare_models,\n    save_model,\n    build_preprocessor,\n    CATBOOST_AVAILABLE,\n    XGBOOST_AVAILABLE,\n    LIGHTGBM_AVAILABLE\n)\nfrom src.features import get_feature_columns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.ensemble import StackingRegressor, VotingRegressor\nfrom sklearn.linear_model import Ridge\n\n# Display settings\npd.set_option('display.max_columns', 50)\npd.set_option('display.max_rows', 100)\npd.set_option('display.float_format', '{:.2f}'.format)\nsns.set_theme(style='whitegrid')\nwarnings.filterwarnings('ignore')\n\nprint(\"Core libraries loaded!\")\nprint(f\"CatBoost available: {CATBOOST_AVAILABLE}\")\nprint(f\"XGBoost available: {XGBOOST_AVAILABLE}\")\nprint(f\"LightGBM available: {LIGHTGBM_AVAILABLE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for optional libraries (Optuna and SHAP)\n# XGBoost and LightGBM availability already checked via src.modeling\n\ntry:\n    import optuna\n    from optuna.samplers import TPESampler  # Import TPESampler\n    OPTUNA_AVAILABLE = True\n    print(f\"Optuna available: {OPTUNA_AVAILABLE} (version {optuna.__version__})\")\nexcept ImportError:\n    OPTUNA_AVAILABLE = False\n    print(\"Optuna not available. Install with: pip install optuna\")\n\ntry:\n    import shap\n    SHAP_AVAILABLE = True\n    print(f\"SHAP available: {SHAP_AVAILABLE}\")\nexcept ImportError:\n    SHAP_AVAILABLE = False\n    print(\"SHAP not available. Install with: pip install shap\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "DATA_DIR = Path('../data')\nTARGET_COL = '_precio_num'\n\n# Load processed features (already has 1-99th percentile outlier removal applied)\ntry:\n    df = pd.read_parquet(DATA_DIR / 'db_features.parquet')\n    print(\"Loaded from parquet\")\nexcept (ImportError, FileNotFoundError):\n    df = pd.read_csv(DATA_DIR / 'db_features.csv')\n    print(\"Loaded from CSV\")\n\nprint(f\"\\nDataset shape: {df.shape}\")\nprint(f\"Rows: {len(df):,}\")\nprint(f\"Columns: {len(df.columns)}\")\n\n# Show price range to confirm outlier removal was applied\nprice_min = df[TARGET_COL].min()\nprice_max = df[TARGET_COL].max()\nprint(f\"\\nPrice range: €{price_min:,.0f} - €{price_max:,.0f}\")\nprint(\"(Note: Data from notebook 02/03 already has 1-99th percentile outlier removal)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature types\n",
    "feature_cols, numeric_cols, categorical_cols = get_feature_columns(df, TARGET_COL)\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"  Numeric: {len(numeric_cols)}\")\n",
    "print(f\"  Categorical: {len(categorical_cols)}\")\n",
    "\n",
    "print(f\"\\nNumeric features:\")\n",
    "for col in numeric_cols:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nCategorical features:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train/test split\n",
    "X = df[feature_cols].copy()\n",
    "y = df[TARGET_COL].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train):,} samples\")\n",
    "print(f\"Test: {len(X_test):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Feature Importance Analysis (CatBoost)\n\nWe use CatBoost (our best performing model) for feature importance analysis:\n1. **CatBoost Native Importance** - Built-in feature importance from the model\n2. **Permutation Importance** - Model-agnostic, reliable measure\n3. **SHAP Values** - For detailed interpretability"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train CatBoost for feature importance analysis (our best model)\nif CATBOOST_AVAILABLE:\n    from catboost import CatBoostRegressor\n    \n    print(\"Training CatBoost for feature importance analysis...\")\n    \n    # Prepare CatBoost data\n    X_cb, y_cb = prepare_catboost_data(df, numeric_cols, categorical_cols, TARGET_COL)\n    X_cb_train, X_cb_test, y_cb_train, y_cb_test = train_test_split(\n        X_cb, y_cb, test_size=0.2, random_state=42\n    )\n    \n    # Train CatBoost\n    cat_baseline = CatBoostRegressor(\n        iterations=300,\n        depth=6,\n        learning_rate=0.1,\n        cat_features=categorical_cols,\n        random_seed=42,\n        verbose=False\n    )\n    cat_baseline.fit(X_cb_train, y_cb_train)\n    \n    y_pred = cat_baseline.predict(X_cb_test)\n    baseline_metrics = evaluate_predictions(y_cb_test, y_pred)\n    \n    print(f\"\\nBaseline Performance (CatBoost, all features):\")\n    print(f\"  RMSE: {baseline_metrics['rmse']:,.2f}\")\n    print(f\"  MAE:  {baseline_metrics['mae']:,.2f}\")\n    print(f\"  R²:   {baseline_metrics['r2']:.4f}\")\nelse:\n    print(\"CatBoost not available - using HistGradientBoosting instead\")\n    hgb_pipeline = build_sklearn_pipeline('hist_gradient_boosting', numeric_cols, categorical_cols)\n    hgb_pipeline.fit(X_train, y_train)\n    y_pred = hgb_pipeline.predict(X_test)\n    baseline_metrics = evaluate_predictions(y_test, y_pred)\n    print(f\"\\nBaseline Performance (HGB, all features):\")\n    print(f\"  RMSE: {baseline_metrics['rmse']:,.2f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Feature Importance using CatBoost's native importance + Permutation Importance\nif CATBOOST_AVAILABLE:\n    print(\"Computing CatBoost feature importance...\")\n    \n    # Method 1: CatBoost's native feature importance (fast)\n    native_importance = cat_baseline.get_feature_importance()\n    \n    # Method 2: Permutation importance (more reliable but slower)\n    print(\"Computing permutation importance (this may take a few minutes)...\")\n    perm_importance = permutation_importance(\n        cat_baseline, X_cb_test, y_cb_test,\n        n_repeats=10,\n        random_state=42,\n        n_jobs=-1\n    )\n    \n    # Create importance DataFrame combining both methods\n    importance_df = pd.DataFrame({\n        'feature': feature_cols,\n        'native_importance': native_importance,\n        'importance_mean': perm_importance.importances_mean,\n        'importance_std': perm_importance.importances_std\n    }).sort_values('importance_mean', ascending=False)\n    \n    print(\"\\n=== CatBoost Feature Importance (Top 20) ===\")\n    display(importance_df.head(20))\nelse:\n    # Fallback to sklearn permutation importance\n    print(\"Computing permutation importance with HGB...\")\n    perm_importance = permutation_importance(\n        hgb_pipeline, X_test, y_test,\n        n_repeats=10, random_state=42, n_jobs=-1\n    )\n    importance_df = pd.DataFrame({\n        'feature': feature_cols,\n        'importance_mean': perm_importance.importances_mean,\n        'importance_std': perm_importance.importances_std\n    }).sort_values('importance_mean', ascending=False)\n    display(importance_df.head(20))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize feature importance (CatBoost native + permutation)\nfig, axes = plt.subplots(1, 3 if CATBOOST_AVAILABLE else 2, figsize=(18 if CATBOOST_AVAILABLE else 14, 8))\n\ntop_n = 20\ntop_features = importance_df.head(top_n)\n\n# Native importance (CatBoost only)\nif CATBOOST_AVAILABLE:\n    ax = axes[0]\n    top_native = importance_df.nlargest(top_n, 'native_importance')\n    ax.barh(range(top_n), top_native['native_importance'], color='steelblue')\n    ax.set_yticks(range(top_n))\n    ax.set_yticklabels(top_native['feature'])\n    ax.set_xlabel('CatBoost Native Importance')\n    ax.set_title(f'Top {top_n} Features (CatBoost Native)')\n    ax.invert_yaxis()\n    perm_ax_idx = 1\nelse:\n    perm_ax_idx = 0\n\n# Permutation importance\nax = axes[perm_ax_idx]\nax.barh(range(top_n), top_features['importance_mean'], \n        xerr=top_features['importance_std'], capsize=3, color='coral')\nax.set_yticks(range(top_n))\nax.set_yticklabels(top_features['feature'])\nax.set_xlabel('Permutation Importance')\nax.set_title(f'Top {top_n} Features (Permutation)')\nax.invert_yaxis()\n\n# Cumulative importance\nax = axes[perm_ax_idx + 1]\nimportance_df_sorted = importance_df.sort_values('importance_mean', ascending=False)\nimportance_df_sorted['cumulative'] = importance_df_sorted['importance_mean'].cumsum()\nimportance_df_sorted['cumulative_pct'] = importance_df_sorted['cumulative'] / importance_df_sorted['importance_mean'].sum() * 100\n\nax.plot(range(len(importance_df_sorted)), importance_df_sorted['cumulative_pct'], marker='o', markersize=3)\nax.axhline(80, color='red', linestyle='--', label='80% threshold')\nax.axhline(90, color='orange', linestyle='--', label='90% threshold')\nax.set_xlabel('Number of Features')\nax.set_ylabel('Cumulative Importance (%)')\nax.set_title('Cumulative Feature Importance')\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Find how many features for 80% and 90% importance\nn_80 = (importance_df_sorted['cumulative_pct'] <= 80).sum() + 1\nn_90 = (importance_df_sorted['cumulative_pct'] <= 90).sum() + 1\nprint(f\"\\nFeatures needed for 80% cumulative importance: {n_80}\")\nprint(f\"Features needed for 90% cumulative importance: {n_90}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify features with near-zero or negative importance\n",
    "low_importance = importance_df[importance_df['importance_mean'] <= 0]\n",
    "print(f\"\\n=== Features with Zero/Negative Importance ({len(low_importance)}) ===\")\n",
    "if len(low_importance) > 0:\n",
    "    display(low_importance)\n",
    "else:\n",
    "    print(\"No features with zero/negative importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 SHAP Values (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE and CATBOOST_AVAILABLE:\n",
    "    print(\"Computing SHAP values with CatBoost...\")\n",
    "    \n",
    "    # Prepare CatBoost data\n",
    "    X_cb, y_cb = prepare_catboost_data(df, numeric_cols, categorical_cols, TARGET_COL)\n",
    "    X_cb_train, X_cb_test, y_cb_train, y_cb_test = train_test_split(\n",
    "        X_cb, y_cb, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train CatBoost\n",
    "    cat_model = build_catboost_model(categorical_cols, loss_function='RMSE')\n",
    "    cat_model.fit(X_cb_train, y_cb_train)\n",
    "    \n",
    "    # Compute SHAP values (use sample for speed)\n",
    "    sample_size = min(500, len(X_cb_test))\n",
    "    X_sample = X_cb_test.sample(sample_size, random_state=42)\n",
    "    \n",
    "    explainer = shap.TreeExplainer(cat_model)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=False)\n",
    "    plt.title(\"SHAP Feature Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"SHAP or CatBoost not available - skipping SHAP analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. User Input Feature Analysis\n",
    "\n",
    "Categorize features by how easy they are for users to provide:\n",
    "- **Easy**: Users know these directly (RAM, SSD, brand, screen size)\n",
    "- **Medium**: Users can look up or select from dropdown (CPU family, GPU series, screen type)\n",
    "- **Hard**: Requires backend lookup (benchmark scores, prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Categorize features by user input difficulty\nuser_easy_features = [\n    '_ram_gb',\n    '_ssd_gb', \n    '_brand',\n    '_tamano_pantalla_pulgadas',\n    '_peso_kg',\n    '_cpu_cores',\n    '_tiene_wifi',\n    '_tiene_bluetooth',\n    '_tiene_webcam',\n]\n\nuser_medium_features = [\n    'cpu_brand',\n    'cpu_family',\n    'gpu_brand',\n    'gpu_series',\n    'gpu_is_integrated',\n    '_serie',\n    '_resolucion_pixeles',\n    '_tasa_refresco_hz',\n    '_gpu_memory_gb',\n    # Screen technology columns (if present)\n    'Pantalla_Tecnología de la pantalla',\n]\n\nuser_hard_features = [\n    # CPU benchmark features (require backend lookup)\n    '_cpu_mark',\n    '_cpu_rank',\n    '_cpu_value',\n    '_cpu_price_usd',\n    # GPU benchmark features (require backend lookup)\n    '_gpu_mark',\n    '_gpu_rank',\n    '_gpu_value',\n    '_gpu_price_usd',\n]\n\n# Filter to only features that exist in our dataset\nuser_easy_features = [f for f in user_easy_features if f in feature_cols]\nuser_medium_features = [f for f in user_medium_features if f in feature_cols]\nuser_hard_features = [f for f in user_hard_features if f in feature_cols]\n\nprint(\"=== Feature Categorization for User Input ===\")\nprint(f\"\\nEASY (user knows directly): {len(user_easy_features)}\")\nfor f in user_easy_features:\n    imp = importance_df[importance_df['feature'] == f]['importance_mean'].values\n    imp_val = imp[0] if len(imp) > 0 else 0\n    print(f\"  - {f}: importance={imp_val:.4f}\")\n\nprint(f\"\\nMEDIUM (dropdown/lookup): {len(user_medium_features)}\")\nfor f in user_medium_features:\n    imp = importance_df[importance_df['feature'] == f]['importance_mean'].values\n    imp_val = imp[0] if len(imp) > 0 else 0\n    print(f\"  - {f}: importance={imp_val:.4f}\")\n\nprint(f\"\\nHARD (backend lookup needed): {len(user_hard_features)}\")\nfor f in user_hard_features:\n    imp = importance_df[importance_df['feature'] == f]['importance_mean'].values\n    imp_val = imp[0] if len(imp) > 0 else 0\n    print(f\"  - {f}: importance={imp_val:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature sets for different model versions\n",
    "# Full model: All features\n",
    "# Lite model: Easy + Medium features only (no backend lookup)\n",
    "\n",
    "lite_features = user_easy_features + user_medium_features\n",
    "lite_numeric = [f for f in lite_features if f in numeric_cols]\n",
    "lite_categorical = [f for f in lite_features if f in categorical_cols]\n",
    "\n",
    "print(f\"\\n=== Feature Sets ===\")\n",
    "print(f\"Full model: {len(feature_cols)} features\")\n",
    "print(f\"Lite model: {len(lite_features)} features (no benchmark lookups)\")\n",
    "print(f\"  - Numeric: {len(lite_numeric)}\")\n",
    "print(f\"  - Categorical: {len(lite_categorical)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare Full vs Lite model performance using CatBoost\nprint(\"\\n=== Comparing Full vs Lite Model (CatBoost) ===\")\n\nif CATBOOST_AVAILABLE:\n    from catboost import CatBoostRegressor\n    \n    # Full model (already trained above as cat_baseline)\n    print(\"\\nFull model (all features):\")\n    full_cv_scores = cross_val_score(\n        CatBoostRegressor(iterations=300, depth=6, learning_rate=0.1,\n                          cat_features=categorical_cols, random_seed=42, verbose=False),\n        X_cb_train, y_cb_train, cv=5, scoring='neg_root_mean_squared_error'\n    )\n    full_rmse = -full_cv_scores.mean()\n    print(f\"  RMSE: {full_rmse:,.2f}\")\n    \n    # Lite model (user-friendly features only)\n    if len(lite_features) > 0:\n        print(\"\\nLite model (user-friendly features only):\")\n        \n        # Prepare lite CatBoost data\n        lite_df = df[lite_features + [TARGET_COL]].copy()\n        X_cb_lite, y_cb_lite = prepare_catboost_data(\n            lite_df, lite_numeric, lite_categorical, TARGET_COL\n        )\n        X_cb_lite_train, _, y_cb_lite_train, _ = train_test_split(\n            X_cb_lite, y_cb_lite, test_size=0.2, random_state=42\n        )\n        \n        lite_cv_scores = cross_val_score(\n            CatBoostRegressor(iterations=300, depth=6, learning_rate=0.1,\n                              cat_features=lite_categorical, random_seed=42, verbose=False),\n            X_cb_lite_train, y_cb_lite_train, cv=5, scoring='neg_root_mean_squared_error'\n        )\n        lite_rmse = -lite_cv_scores.mean()\n        print(f\"  RMSE: {lite_rmse:,.2f}\")\n        \n        # Performance difference\n        rmse_increase = (lite_rmse - full_rmse) / full_rmse * 100\n        print(f\"\\nPerformance difference:\")\n        print(f\"  RMSE increase: {rmse_increase:+.1f}%\")\n        print(f\"  (Lite model uses {len(lite_features)} features vs {len(feature_cols)} in full model)\")\nelse:\n    # Fallback to HGB if CatBoost not available\n    print(\"CatBoost not available - using HistGradientBoosting\")\n    full_pipeline = build_sklearn_pipeline('hist_gradient_boosting', numeric_cols, categorical_cols)\n    full_results = evaluate_sklearn_cv(full_pipeline, X_train, y_train, cv=5)\n    print(f\"Full Model - RMSE: {full_results['rmse_mean']:,.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection\n",
    "\n",
    "Select top features based on importance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features with positive importance\n",
    "important_features = importance_df[importance_df['importance_mean'] > 0]['feature'].tolist()\n",
    "\n",
    "# Or select top N features\n",
    "TOP_N_FEATURES = min(30, len(important_features))  # Adjust as needed\n",
    "top_features_list = importance_df.head(TOP_N_FEATURES)['feature'].tolist()\n",
    "\n",
    "print(f\"\\n=== Feature Selection ===\")\n",
    "print(f\"Features with positive importance: {len(important_features)}\")\n",
    "print(f\"Selected top {TOP_N_FEATURES} features for optimization\")\n",
    "\n",
    "# Split into numeric and categorical\n",
    "selected_numeric = [f for f in top_features_list if f in numeric_cols]\n",
    "selected_categorical = [f for f in top_features_list if f in categorical_cols]\n",
    "\n",
    "print(f\"  - Numeric: {len(selected_numeric)}\")\n",
    "print(f\"  - Categorical: {len(selected_categorical)}\")\n",
    "\n",
    "# Prepare selected feature data\n",
    "X_train_selected = X_train[top_features_list]\n",
    "X_test_selected = X_test[top_features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify performance with selected features using CatBoost\nprint(\"\\nVerifying performance with selected features (CatBoost)...\")\n\nif CATBOOST_AVAILABLE:\n    # Prepare selected feature data for CatBoost\n    selected_df = df[top_features_list + [TARGET_COL]].copy()\n    X_cb_selected, y_cb_selected = prepare_catboost_data(\n        selected_df, selected_numeric, selected_categorical, TARGET_COL\n    )\n    X_cb_sel_train, X_cb_sel_test, y_cb_sel_train, y_cb_sel_test = train_test_split(\n        X_cb_selected, y_cb_selected, test_size=0.2, random_state=42\n    )\n    \n    selected_cv_scores = cross_val_score(\n        CatBoostRegressor(iterations=300, depth=6, learning_rate=0.1,\n                          cat_features=selected_categorical, random_seed=42, verbose=False),\n        X_cb_sel_train, y_cb_sel_train, cv=5, scoring='neg_root_mean_squared_error'\n    )\n    selected_rmse = -selected_cv_scores.mean()\n    \n    print(f\"\\nAll features ({len(feature_cols)}):\")\n    print(f\"  RMSE: {full_rmse:,.2f}\")\n    \n    print(f\"\\nSelected features ({len(top_features_list)}):\")\n    print(f\"  RMSE: {selected_rmse:,.2f}\")\n    \n    rmse_diff = (selected_rmse - full_rmse) / full_rmse * 100\n    print(f\"\\nRMSE change: {rmse_diff:+.1f}%\")\nelse:\n    selected_pipeline = build_sklearn_pipeline('hist_gradient_boosting', selected_numeric, selected_categorical)\n    selected_results = evaluate_sklearn_cv(selected_pipeline, X_train_selected, y_train, cv=5)\n    print(f\"Selected features RMSE: {selected_results['rmse_mean']:,.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Additional Models\n",
    "\n",
    "Add XGBoost and LightGBM to our model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "# Use selected features for all models\n",
    "X_opt = X_train_selected\n",
    "X_opt_test = X_test_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HistGradientBoosting (baseline)\n",
    "print(\"=\" * 60)\n",
    "print(\"HIST GRADIENT BOOSTING (baseline)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "hgb_pipeline = build_sklearn_pipeline('hist_gradient_boosting', selected_numeric, selected_categorical)\n",
    "hgb_results = evaluate_sklearn_cv(hgb_pipeline, X_opt, y_train, cv=5)\n",
    "all_results['HistGradientBoosting'] = hgb_results\n",
    "\n",
    "print(f\"RMSE: {hgb_results['rmse_mean']:,.2f} (+/- {hgb_results['rmse_std']:,.2f})\")\n",
    "print(f\"R²: {hgb_results['r2_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "print(\"=\" * 60)\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rf_pipeline = build_sklearn_pipeline('random_forest', selected_numeric, selected_categorical)\n",
    "rf_results = evaluate_sklearn_cv(rf_pipeline, X_opt, y_train, cv=5)\n",
    "all_results['Random Forest'] = rf_results\n",
    "\n",
    "print(f\"RMSE: {rf_results['rmse_mean']:,.2f} (+/- {rf_results['rmse_std']:,.2f})\")\n",
    "print(f\"R²: {rf_results['r2_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression\n",
    "print(\"=\" * 60)\n",
    "print(\"RIDGE REGRESSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ridge_pipeline = build_sklearn_pipeline('ridge', selected_numeric, selected_categorical)\n",
    "ridge_results = evaluate_sklearn_cv(ridge_pipeline, X_opt, y_train, cv=5)\n",
    "all_results['Ridge'] = ridge_results\n",
    "\n",
    "print(f\"RMSE: {ridge_results['rmse_mean']:,.2f} (+/- {ridge_results['rmse_std']:,.2f})\")\n",
    "print(f\"R²: {ridge_results['r2_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "if XGBOOST_AVAILABLE:\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from src.modeling import build_preprocessor\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"XGBOOST\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    preprocessor = build_preprocessor(selected_numeric, selected_categorical)\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        enable_categorical=False  # We handle categoricals via preprocessing\n",
    "    )\n",
    "    \n",
    "    xgb_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', xgb_model)\n",
    "    ])\n",
    "    \n",
    "    xgb_results = evaluate_sklearn_cv(xgb_pipeline, X_opt, y_train, cv=5)\n",
    "    all_results['XGBoost'] = xgb_results\n",
    "    \n",
    "    print(f\"RMSE: {xgb_results['rmse_mean']:,.2f} (+/- {xgb_results['rmse_std']:,.2f})\")\n",
    "    print(f\"R²: {xgb_results['r2_mean']:.4f}\")\n",
    "else:\n",
    "    print(\"XGBoost not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from src.modeling import build_preprocessor\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"LIGHTGBM\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    preprocessor = build_preprocessor(selected_numeric, selected_categorical)\n",
    "    lgb_model = LGBMRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    lgb_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', lgb_model)\n",
    "    ])\n",
    "    \n",
    "    lgb_results = evaluate_sklearn_cv(lgb_pipeline, X_opt, y_train, cv=5)\n",
    "    all_results['LightGBM'] = lgb_results\n",
    "    \n",
    "    print(f\"RMSE: {lgb_results['rmse_mean']:,.2f} (+/- {lgb_results['rmse_std']:,.2f})\")\n",
    "    print(f\"R²: {lgb_results['r2_mean']:.4f}\")\n",
    "else:\n",
    "    print(\"LightGBM not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CatBoost\nif CATBOOST_AVAILABLE:\n    from catboost import CatBoostRegressor\n    \n    print(\"=\" * 60)\n    print(\"CATBOOST\")\n    print(\"=\" * 60)\n    \n    # Prepare CatBoost data with selected features\n    X_cb_opt, y_cb_opt = prepare_catboost_data(\n        df[top_features_list + [TARGET_COL]].copy(),\n        selected_numeric,\n        selected_categorical,\n        TARGET_COL\n    )\n    \n    X_cb_train, X_cb_test, y_cb_train, y_cb_test = train_test_split(\n        X_cb_opt, y_cb_opt, test_size=0.2, random_state=42\n    )\n    \n    cat_model = build_catboost_model(selected_categorical, loss_function='RMSE')\n    \n    # Compute all metrics via CV\n    neg_rmse_scores = cross_val_score(\n        cat_model, X_cb_train, y_cb_train,\n        cv=5, scoring='neg_root_mean_squared_error'\n    )\n    neg_mae_scores = cross_val_score(\n        cat_model, X_cb_train, y_cb_train,\n        cv=5, scoring='neg_mean_absolute_error'\n    )\n    r2_scores = cross_val_score(\n        cat_model, X_cb_train, y_cb_train,\n        cv=5, scoring='r2'\n    )\n    \n    cat_results = {\n        'rmse_mean': -neg_rmse_scores.mean(),\n        'rmse_std': neg_rmse_scores.std(),\n        'mae_mean': -neg_mae_scores.mean(),\n        'mae_std': neg_mae_scores.std(),\n        'r2_mean': r2_scores.mean(),\n        'r2_std': r2_scores.std(),\n    }\n    all_results['CatBoost'] = cat_results\n    \n    print(f\"RMSE: {cat_results['rmse_mean']:,.2f} (+/- {cat_results['rmse_std']:,.2f})\")\n    print(f\"MAE:  {cat_results['mae_mean']:,.2f} (+/- {cat_results['mae_std']:,.2f})\")\n    print(f\"R²:   {cat_results['r2_mean']:.4f}\")\nelse:\n    print(\"CatBoost not available\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "comparison = compare_models(all_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON (sorted by RMSE)\")\n",
    "print(\"=\" * 80)\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 6.1 Preprocessing Strategy Comparison (Ablation Study)\n\nCompare different outlier removal and target transformation strategies using CatBoost.\n\n**Strategies tested:**\n1. **1-99th percentile** (current) - Remove top/bottom 1%\n2. **2-98th percentile** - Remove top/bottom 2%  \n3. **IQR method** - Remove beyond 1.5×IQR (more aggressive)\n4. **Log-transform** - No outlier removal, transform target instead",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Preprocessing Strategy Comparison\n# IMPORTANT: Calculate thresholds on TRAINING data only to avoid data leakage\n\nif CATBOOST_AVAILABLE:\n    from catboost import CatBoostRegressor\n    \n    # Load raw data (before any outlier removal)\n    try:\n        df_raw = pd.read_parquet(DATA_DIR / 'db_features_raw.parquet')\n    except (ImportError, FileNotFoundError):\n        # If no raw file, we'll use the current df and note this\n        print(\"Warning: Using current df. For proper comparison, save raw features before outlier removal.\")\n        df_raw = df.copy()\n    \n    # Get feature columns from raw data\n    raw_feature_cols = [c for c in df_raw.columns if c in feature_cols]\n    raw_numeric = [c for c in raw_feature_cols if c in numeric_cols]\n    raw_categorical = [c for c in raw_feature_cols if c in categorical_cols]\n    \n    print(\"=\" * 70)\n    print(\"PREPROCESSING STRATEGY COMPARISON (CatBoost)\")\n    print(\"=\" * 70)\n    print(f\"\\nOriginal dataset size: {len(df_raw):,} rows\")\n    \n    results_preprocessing = {}\n    \n    # Helper function to train and evaluate\n    def evaluate_strategy(X_train, X_test, y_train, y_test, strategy_name, use_log=False):\n        \"\"\"Train CatBoost and evaluate with proper handling of log-transform.\"\"\"\n        \n        if use_log:\n            y_train_model = np.log1p(y_train)\n            y_test_model = np.log1p(y_test)\n        else:\n            y_train_model = y_train\n            y_test_model = y_test\n        \n        # Train CatBoost\n        model = CatBoostRegressor(\n            iterations=300,\n            depth=6,\n            learning_rate=0.1,\n            cat_features=raw_categorical,\n            random_seed=42,\n            verbose=False\n        )\n        model.fit(X_train, y_train_model)\n        \n        # Predict\n        y_pred_model = model.predict(X_test)\n        \n        if use_log:\n            # Transform back to original scale\n            y_pred = np.expm1(y_pred_model)\n            y_actual = y_test  # Original scale for metrics\n        else:\n            y_pred = y_pred_model\n            y_actual = y_test\n        \n        # Calculate metrics on original scale\n        metrics = evaluate_predictions(y_actual, y_pred)\n        return metrics, len(X_train)\n    \n    print(\"\\nRunning experiments (this may take a few minutes)...\")\n    print(\"-\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Strategy 1: 1-99th Percentile (Current baseline)\nprint(\"\\n1. Testing 1-99th Percentile...\")\n\n# Remove rows without target first\ndf_valid = df_raw[df_raw[TARGET_COL].notna()].copy()\n\n# IMPORTANT: Split FIRST, then calculate thresholds on training data only\nX_all = df_valid[raw_feature_cols].copy()\ny_all = df_valid[TARGET_COL].copy()\n\n# Same random state for all experiments\nX_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n    X_all, y_all, test_size=0.2, random_state=42\n)\n\n# Calculate percentiles on TRAINING data only (avoid data leakage!)\np1_train = y_train_full.quantile(0.01)\np99_train = y_train_full.quantile(0.99)\n\n# Filter training data\ntrain_mask = (y_train_full >= p1_train) & (y_train_full <= p99_train)\nX_train_1_99 = X_train_full[train_mask]\ny_train_1_99 = y_train_full[train_mask]\n\n# Filter test data using SAME thresholds (from training)\ntest_mask = (y_test_full >= p1_train) & (y_test_full <= p99_train)\nX_test_1_99 = X_test_full[test_mask]\ny_test_1_99 = y_test_full[test_mask]\n\n# Prepare CatBoost data\nX_train_cb, _ = prepare_catboost_data(\n    pd.concat([X_train_1_99, y_train_1_99], axis=1), \n    raw_numeric, raw_categorical, TARGET_COL\n)\nX_test_cb, _ = prepare_catboost_data(\n    pd.concat([X_test_1_99, y_test_1_99], axis=1), \n    raw_numeric, raw_categorical, TARGET_COL\n)\n\nmetrics_1_99, n_train_1_99 = evaluate_strategy(\n    X_train_cb, X_test_cb, y_train_1_99, y_test_1_99, \"1-99th Percentile\"\n)\nresults_preprocessing['1-99th Percentile'] = {**metrics_1_99, 'n_train': n_train_1_99}\n\nprint(f\"   Train samples: {n_train_1_99:,}, Price range: €{p1_train:,.0f} - €{p99_train:,.0f}\")\nprint(f\"   RMSE: {metrics_1_99['rmse']:,.2f}, MAE: {metrics_1_99['mae']:,.2f}, R²: {metrics_1_99['r2']:.4f}, MAPE: {metrics_1_99['mape']:.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Strategy 2: 2-98th Percentile\nprint(\"\\n2. Testing 2-98th Percentile...\")\n\np2_train = y_train_full.quantile(0.02)\np98_train = y_train_full.quantile(0.98)\n\ntrain_mask = (y_train_full >= p2_train) & (y_train_full <= p98_train)\nX_train_2_98 = X_train_full[train_mask]\ny_train_2_98 = y_train_full[train_mask]\n\ntest_mask = (y_test_full >= p2_train) & (y_test_full <= p98_train)\nX_test_2_98 = X_test_full[test_mask]\ny_test_2_98 = y_test_full[test_mask]\n\nX_train_cb, _ = prepare_catboost_data(\n    pd.concat([X_train_2_98, y_train_2_98], axis=1), \n    raw_numeric, raw_categorical, TARGET_COL\n)\nX_test_cb, _ = prepare_catboost_data(\n    pd.concat([X_test_2_98, y_test_2_98], axis=1), \n    raw_numeric, raw_categorical, TARGET_COL\n)\n\nmetrics_2_98, n_train_2_98 = evaluate_strategy(\n    X_train_cb, X_test_cb, y_train_2_98, y_test_2_98, \"2-98th Percentile\"\n)\nresults_preprocessing['2-98th Percentile'] = {**metrics_2_98, 'n_train': n_train_2_98}\n\nprint(f\"   Train samples: {n_train_2_98:,}, Price range: €{p2_train:,.0f} - €{p98_train:,.0f}\")\nprint(f\"   RMSE: {metrics_2_98['rmse']:,.2f}, MAE: {metrics_2_98['mae']:,.2f}, R²: {metrics_2_98['r2']:.4f}, MAPE: {metrics_2_98['mape']:.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Strategy 3: IQR Method (1.5 * IQR)\nprint(\"\\n3. Testing IQR Method (1.5 × IQR)...\")\n\nQ1_train = y_train_full.quantile(0.25)\nQ3_train = y_train_full.quantile(0.75)\nIQR_train = Q3_train - Q1_train\niqr_lower = Q1_train - 1.5 * IQR_train\niqr_upper = Q3_train + 1.5 * IQR_train\n\n# Ensure lower bound is not negative (prices can't be negative)\niqr_lower = max(0, iqr_lower)\n\ntrain_mask = (y_train_full >= iqr_lower) & (y_train_full <= iqr_upper)\nX_train_iqr = X_train_full[train_mask]\ny_train_iqr = y_train_full[train_mask]\n\ntest_mask = (y_test_full >= iqr_lower) & (y_test_full <= iqr_upper)\nX_test_iqr = X_test_full[test_mask]\ny_test_iqr = y_test_full[test_mask]\n\nX_train_cb, _ = prepare_catboost_data(\n    pd.concat([X_train_iqr, y_train_iqr], axis=1), \n    raw_numeric, raw_categorical, TARGET_COL\n)\nX_test_cb, _ = prepare_catboost_data(\n    pd.concat([X_test_iqr, y_test_iqr], axis=1), \n    raw_numeric, raw_categorical, TARGET_COL\n)\n\nmetrics_iqr, n_train_iqr = evaluate_strategy(\n    X_train_cb, X_test_cb, y_train_iqr, y_test_iqr, \"IQR Method\"\n)\nresults_preprocessing['IQR Method'] = {**metrics_iqr, 'n_train': n_train_iqr}\n\nprint(f\"   Train samples: {n_train_iqr:,}, Price range: €{iqr_lower:,.0f} - €{iqr_upper:,.0f}\")\nprint(f\"   RMSE: {metrics_iqr['rmse']:,.2f}, MAE: {metrics_iqr['mae']:,.2f}, R²: {metrics_iqr['r2']:.4f}, MAPE: {metrics_iqr['mape']:.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Strategy 4: Log-Transform (No outlier removal)\nprint(\"\\n4. Testing Log-Transform (no outlier removal)...\")\n\n# Use ALL data (no outlier removal)\nX_train_cb, _ = prepare_catboost_data(\n    pd.concat([X_train_full, y_train_full], axis=1), \n    raw_numeric, raw_categorical, TARGET_COL\n)\nX_test_cb, _ = prepare_catboost_data(\n    pd.concat([X_test_full, y_test_full], axis=1), \n    raw_numeric, raw_categorical, TARGET_COL\n)\n\nmetrics_log, n_train_log = evaluate_strategy(\n    X_train_cb, X_test_cb, y_train_full, y_test_full, \"Log-Transform\", use_log=True\n)\nresults_preprocessing['Log-Transform'] = {**metrics_log, 'n_train': n_train_log}\n\nprint(f\"   Train samples: {n_train_log:,} (all data, no removal)\")\nprint(f\"   RMSE: {metrics_log['rmse']:,.2f}, MAE: {metrics_log['mae']:,.2f}, R²: {metrics_log['r2']:.4f}, MAPE: {metrics_log['mape']:.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Bonus: Log-Transform + 1-99th Percentile (best of both?)\nprint(\"\\n5. Testing Log-Transform + 1-99th Percentile...\")\n\nX_train_cb, _ = prepare_catboost_data(\n    pd.concat([X_train_1_99, y_train_1_99], axis=1), \n    raw_numeric, raw_categorical, TARGET_COL\n)\nX_test_cb, _ = prepare_catboost_data(\n    pd.concat([X_test_1_99, y_test_1_99], axis=1), \n    raw_numeric, raw_categorical, TARGET_COL\n)\n\nmetrics_log_pct, n_train_log_pct = evaluate_strategy(\n    X_train_cb, X_test_cb, y_train_1_99, y_test_1_99, \"Log + 1-99th\", use_log=True\n)\nresults_preprocessing['Log + 1-99th Pct'] = {**metrics_log_pct, 'n_train': n_train_log_pct}\n\nprint(f\"   Train samples: {n_train_log_pct:,}\")\nprint(f\"   RMSE: {metrics_log_pct['rmse']:,.2f}, MAE: {metrics_log_pct['mae']:,.2f}, R²: {metrics_log_pct['r2']:.4f}, MAPE: {metrics_log_pct['mape']:.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Summary comparison table\nprint(\"\\n\" + \"=\" * 90)\nprint(\"PREPROCESSING STRATEGY COMPARISON SUMMARY\")\nprint(\"=\" * 90)\n\ncomparison_df = pd.DataFrame(results_preprocessing).T\ncomparison_df = comparison_df[['n_train', 'rmse', 'mae', 'r2', 'mape']]\ncomparison_df.columns = ['Train Samples', 'RMSE (€)', 'MAE (€)', 'R²', 'MAPE (%)']\ncomparison_df = comparison_df.sort_values('RMSE (€)')\n\nprint(\"\\n\")\ndisplay(comparison_df.round(2))\n\n# Visualization\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nstrategies = comparison_df.index.tolist()\ncolors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c', '#f39c12']\n\n# RMSE comparison\nax = axes[0]\nbars = ax.barh(strategies, comparison_df['RMSE (€)'], color=colors[:len(strategies)])\nax.set_xlabel('RMSE (€)')\nax.set_title('RMSE by Strategy (lower is better)')\nax.invert_yaxis()\nfor i, v in enumerate(comparison_df['RMSE (€)']):\n    ax.text(v + 5, i, f'€{v:,.0f}', va='center', fontsize=10)\n\n# MAPE comparison  \nax = axes[1]\nbars = ax.barh(strategies, comparison_df['MAPE (%)'], color=colors[:len(strategies)])\nax.set_xlabel('MAPE (%)')\nax.set_title('MAPE by Strategy (lower is better)')\nax.invert_yaxis()\nfor i, v in enumerate(comparison_df['MAPE (%)']):\n    ax.text(v + 0.3, i, f'{v:.1f}%', va='center', fontsize=10)\n\n# Trade-off: Samples vs Performance\nax = axes[2]\nax.scatter(comparison_df['Train Samples'], comparison_df['RMSE (€)'], \n           s=150, c=colors[:len(strategies)], edgecolors='black', linewidths=2)\nfor i, strategy in enumerate(strategies):\n    ax.annotate(strategy, (comparison_df.loc[strategy, 'Train Samples'], \n                           comparison_df.loc[strategy, 'RMSE (€)']),\n                xytext=(5, 5), textcoords='offset points', fontsize=9)\nax.set_xlabel('Training Samples')\nax.set_ylabel('RMSE (€)')\nax.set_title('Trade-off: Data Size vs Performance')\n\nplt.tight_layout()\nplt.show()\n\n# Recommendation\nbest_strategy = comparison_df.index[0]\nprint(f\"\\n✓ Best performing strategy: {best_strategy}\")\nprint(f\"  RMSE: €{comparison_df.loc[best_strategy, 'RMSE (€)']:,.2f}\")\nprint(f\"  MAPE: {comparison_df.loc[best_strategy, 'MAPE (%)']:.2f}%\")\nprint(f\"  Training samples: {comparison_df.loc[best_strategy, 'Train Samples']:,.0f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Hyperparameter Tuning with Optuna\n\nTune CatBoost using the **best preprocessing strategy from the experiment above**.\n\nBased on the comparison, we use the strategy with the lowest RMSE for hyperparameter tuning."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Skip HGB tuning - focusing only on CatBoost which performed best\nstudy_hgb = None\nprint(\"Skipping HistGradientBoosting tuning - focusing on CatBoost\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Skip XGBoost tuning - focusing only on CatBoost which performed best\nstudy_xgb = None\nprint(\"Skipping XGBoost tuning - focusing on CatBoost\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hyperparameter tuning using the BEST preprocessing strategy from experiment\nif OPTUNA_AVAILABLE and CATBOOST_AVAILABLE:\n    from catboost import CatBoostRegressor\n    from optuna.samplers import TPESampler\n    \n    # Determine best strategy from experiment results\n    best_strategy = comparison_df.index[0]\n    print(f\"Using best strategy from experiment: {best_strategy}\")\n    print(f\"  RMSE: €{comparison_df.loc[best_strategy, 'RMSE (€)']:,.2f}\")\n    \n    # Select the appropriate training data based on best strategy\n    if 'IQR' in best_strategy:\n        X_tune_train = X_train_iqr\n        y_tune_train = y_train_iqr\n        X_tune_test = X_test_iqr\n        y_tune_test = y_test_iqr\n        print(f\"  Using IQR-filtered data: {len(X_tune_train):,} training samples\")\n    elif '2-98' in best_strategy:\n        X_tune_train = X_train_2_98\n        y_tune_train = y_train_2_98\n        X_tune_test = X_test_2_98\n        y_tune_test = y_test_2_98\n        print(f\"  Using 2-98th percentile data: {len(X_tune_train):,} training samples\")\n    elif '1-99' in best_strategy and 'Log' not in best_strategy:\n        X_tune_train = X_train_1_99\n        y_tune_train = y_train_1_99\n        X_tune_test = X_test_1_99\n        y_tune_test = y_test_1_99\n        print(f\"  Using 1-99th percentile data: {len(X_tune_train):,} training samples\")\n    else:\n        # Default to 1-99th\n        X_tune_train = X_train_1_99\n        y_tune_train = y_train_1_99\n        X_tune_test = X_test_1_99\n        y_tune_test = y_test_1_99\n        print(f\"  Using 1-99th percentile data (default): {len(X_tune_train):,} training samples\")\n    \n    # Prepare CatBoost data for tuning\n    X_tune_cb, _ = prepare_catboost_data(\n        pd.concat([X_tune_train, y_tune_train], axis=1), \n        raw_numeric, raw_categorical, TARGET_COL\n    )\n    \n    def objective_catboost(trial):\n        \"\"\"Objective for CatBoost hyperparameter optimization\"\"\"\n        params = {\n            'iterations': trial.suggest_int('iterations', 200, 800),\n            'depth': trial.suggest_int('depth', 4, 10),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 30),\n        }\n        \n        model = CatBoostRegressor(\n            **params,\n            cat_features=raw_categorical,\n            random_seed=42,\n            verbose=False\n        )\n        \n        scores = cross_val_score(\n            model, X_tune_cb, y_tune_train,\n            cv=3, scoring='neg_root_mean_squared_error'\n        )\n        return -scores.mean()\n    \n    print(\"\\nTuning CatBoost with Optuna (30 trials)...\")\n    study_cat = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n    study_cat.optimize(objective_catboost, n_trials=30, show_progress_bar=True)\n    \n    print(f\"\\nBest CatBoost params: {study_cat.best_params}\")\n    print(f\"Best CatBoost RMSE: {study_cat.best_value:,.2f}\")\nelse:\n    study_cat = None\n    print(\"CatBoost or Optuna not available\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ensemble Methods\n",
    "\n",
    "Combine the best models using stacking or weighted averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tuned models\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.modeling import build_preprocessor\n",
    "\n",
    "tuned_models = {}\n",
    "\n",
    "# Tuned HGB\n",
    "if study_hgb is not None:\n",
    "    tuned_hgb = build_sklearn_pipeline(\n",
    "        'hist_gradient_boosting',\n",
    "        selected_numeric,\n",
    "        selected_categorical,\n",
    "        **study_hgb.best_params\n",
    "    )\n",
    "    tuned_models['HGB'] = tuned_hgb\n",
    "else:\n",
    "    tuned_models['HGB'] = build_sklearn_pipeline(\n",
    "        'hist_gradient_boosting', selected_numeric, selected_categorical\n",
    "    )\n",
    "\n",
    "# Tuned XGBoost\n",
    "if XGBOOST_AVAILABLE:\n",
    "    from xgboost import XGBRegressor\n",
    "    preprocessor = build_preprocessor(selected_numeric, selected_categorical)\n",
    "    \n",
    "    if study_xgb is not None:\n",
    "        xgb_params = study_xgb.best_params\n",
    "    else:\n",
    "        xgb_params = {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.1}\n",
    "    \n",
    "    tuned_xgb = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', XGBRegressor(**xgb_params, random_state=42, n_jobs=-1))\n",
    "    ])\n",
    "    tuned_models['XGB'] = tuned_xgb\n",
    "\n",
    "# Random Forest (not tuned, but included for diversity)\n",
    "tuned_models['RF'] = build_sklearn_pipeline('random_forest', selected_numeric, selected_categorical)\n",
    "\n",
    "print(f\"Prepared {len(tuned_models)} models for ensemble: {list(tuned_models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking Ensemble\n",
    "print(\"=\" * 60)\n",
    "print(\"STACKING ENSEMBLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create estimators list for stacking\n",
    "estimators = [(name, model) for name, model in tuned_models.items()]\n",
    "\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=Ridge(alpha=1.0),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Evaluate stacking\n",
    "stacking_scores = cross_val_score(\n",
    "    stacking_regressor, X_opt, y_train,\n",
    "    cv=5, scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "stacking_results = {\n",
    "    'rmse_mean': -stacking_scores.mean(),\n",
    "    'rmse_std': stacking_scores.std(),\n",
    "    'mae_mean': 0,\n",
    "    'mae_std': 0,\n",
    "    'r2_mean': 0,\n",
    "    'r2_std': 0,\n",
    "}\n",
    "\n",
    "print(f\"Stacking RMSE: {stacking_results['rmse_mean']:,.2f} (+/- {stacking_results['rmse_std']:,.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Ensemble (simple averaging)\n",
    "print(\"=\" * 60)\n",
    "print(\"VOTING ENSEMBLE (Average)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "voting_regressor = VotingRegressor(\n",
    "    estimators=estimators,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "voting_scores = cross_val_score(\n",
    "    voting_regressor, X_opt, y_train,\n",
    "    cv=5, scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "voting_results = {\n",
    "    'rmse_mean': -voting_scores.mean(),\n",
    "    'rmse_std': voting_scores.std(),\n",
    "    'mae_mean': 0,\n",
    "    'mae_std': 0,\n",
    "    'r2_mean': 0,\n",
    "    'r2_std': 0,\n",
    "}\n",
    "\n",
    "print(f\"Voting RMSE: {voting_results['rmse_mean']:,.2f} (+/- {voting_results['rmse_std']:,.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ensemble results to comparison\n",
    "all_results['Stacking Ensemble'] = stacking_results\n",
    "all_results['Voting Ensemble'] = voting_results\n",
    "\n",
    "# Final comparison\n",
    "final_comparison = compare_models(all_results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL MODEL COMPARISON (including ensembles)\")\n",
    "print(\"=\" * 80)\n",
    "display(final_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "best_model_name = final_comparison.iloc[0]['Model']\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "\n",
    "# Train best model on full training data\n",
    "if 'Stacking' in best_model_name:\n",
    "    final_model = stacking_regressor\n",
    "elif 'Voting' in best_model_name:\n",
    "    final_model = voting_regressor\n",
    "elif 'CatBoost' in best_model_name and CATBOOST_AVAILABLE:\n",
    "    if study_cat is not None:\n",
    "        final_model = CatBoostRegressor(\n",
    "            **study_cat.best_params,\n",
    "            cat_features=selected_categorical,\n",
    "            random_seed=42,\n",
    "            verbose=False\n",
    "        )\n",
    "    else:\n",
    "        final_model = build_catboost_model(selected_categorical, loss_function='RMSE')\n",
    "elif best_model_name in tuned_models:\n",
    "    final_model = tuned_models[best_model_name]\n",
    "else:\n",
    "    # Default to HGB\n",
    "    final_model = tuned_models.get('HGB', build_sklearn_pipeline(\n",
    "        'hist_gradient_boosting', selected_numeric, selected_categorical\n",
    "    ))\n",
    "\n",
    "# Fit on training data\n",
    "if 'CatBoost' in best_model_name and CATBOOST_AVAILABLE:\n",
    "    final_model.fit(X_cb_train, y_cb_train)\n",
    "    y_pred = final_model.predict(X_cb_test)\n",
    "    final_metrics = evaluate_predictions(y_cb_test, y_pred)\n",
    "else:\n",
    "    final_model.fit(X_opt, y_train)\n",
    "    y_pred = final_model.predict(X_opt_test)\n",
    "    final_metrics = evaluate_predictions(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n=== Test Set Performance ===\")\n",
    "print(f\"RMSE: {final_metrics['rmse']:,.2f}\")\n",
    "print(f\"MAE:  {final_metrics['mae']:,.2f}\")\n",
    "print(f\"R²:   {final_metrics['r2']:.4f}\")\n",
    "print(f\"MAPE: {final_metrics['mape']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "if 'CatBoost' in best_model_name and CATBOOST_AVAILABLE:\n",
    "    y_actual = y_cb_test\n",
    "else:\n",
    "    y_actual = y_test\n",
    "\n",
    "# Actual vs Predicted\n",
    "ax = axes[0]\n",
    "ax.scatter(y_actual, y_pred, alpha=0.3, s=10)\n",
    "ax.plot([0, y_actual.max()], [0, y_actual.max()], 'r--', lw=2, label='Perfect')\n",
    "ax.set_xlabel('Actual Price (€)')\n",
    "ax.set_ylabel('Predicted Price (€)')\n",
    "ax.set_title(f'{best_model_name}: Actual vs Predicted')\n",
    "ax.legend()\n",
    "\n",
    "# Residuals\n",
    "ax = axes[1]\n",
    "residuals = y_actual - y_pred\n",
    "ax.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(0, color='red', linestyle='--', lw=2)\n",
    "ax.set_xlabel('Residual (€)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(f'Residuals (Mean: {residuals.mean():,.0f}€, Std: {residuals.std():,.0f}€)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata about feature selection and tuning\n",
    "metadata = {\n",
    "    'model_type': best_model_name,\n",
    "    'feature_cols': top_features_list,\n",
    "    'numeric_cols': selected_numeric,\n",
    "    'categorical_cols': selected_categorical,\n",
    "    'target_col': TARGET_COL,\n",
    "    'test_metrics': final_metrics,\n",
    "    'n_features_original': len(feature_cols),\n",
    "    'n_features_selected': len(top_features_list),\n",
    "    'tuning_performed': OPTUNA_AVAILABLE,\n",
    "}\n",
    "\n",
    "# Add tuning params if available\n",
    "if study_hgb is not None:\n",
    "    metadata['hgb_best_params'] = study_hgb.best_params\n",
    "if study_xgb is not None:\n",
    "    metadata['xgb_best_params'] = study_xgb.best_params\n",
    "if study_cat is not None:\n",
    "    metadata['catboost_best_params'] = study_cat.best_params\n",
    "\n",
    "save_model(final_model, '../models/price_model_optimized.pkl', metadata)\n",
    "\n",
    "print(\"\\nModel saved with metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"  {key}: {len(value)} items\")\n",
    "    elif isinstance(value, dict):\n",
    "        print(f\"  {key}: {len(value)} params\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save the lite model (user-friendly features only)\n",
    "if len(lite_features) > 0:\n",
    "    print(\"\\n=== Training and Saving Lite Model ===\")\n",
    "    \n",
    "    lite_model = build_sklearn_pipeline('hist_gradient_boosting', lite_numeric, lite_categorical)\n",
    "    lite_model.fit(X_train[lite_features], y_train)\n",
    "    \n",
    "    y_pred_lite = lite_model.predict(X_test[lite_features])\n",
    "    lite_metrics = evaluate_predictions(y_test, y_pred_lite)\n",
    "    \n",
    "    lite_metadata = {\n",
    "        'model_type': 'HistGradientBoosting (Lite)',\n",
    "        'feature_cols': lite_features,\n",
    "        'numeric_cols': lite_numeric,\n",
    "        'categorical_cols': lite_categorical,\n",
    "        'target_col': TARGET_COL,\n",
    "        'test_metrics': lite_metrics,\n",
    "        'description': 'User-friendly features only (no benchmark lookups)',\n",
    "    }\n",
    "    \n",
    "    save_model(lite_model, '../models/price_model_lite.pkl', lite_metadata)\n",
    "    \n",
    "    print(f\"\\nLite Model Performance:\")\n",
    "    print(f\"  RMSE: {lite_metrics['rmse']:,.2f}\")\n",
    "    print(f\"  R²:   {lite_metrics['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Optimization Results\n",
    "\n",
    "1. **Feature Selection**: Reduced features from original count to top N based on permutation importance\n",
    "2. **Additional Models**: Compared XGBoost, LightGBM with existing models\n",
    "3. **Hyperparameter Tuning**: Used Optuna to tune top models\n",
    "4. **Ensemble Methods**: Tested Stacking and Voting ensembles\n",
    "\n",
    "### Models Saved\n",
    "\n",
    "1. `models/price_model_optimized.pkl` - Best performing model with selected features\n",
    "2. `models/price_model_lite.pkl` - User-friendly model (no benchmark lookups needed)\n",
    "\n",
    "### User Input Features (Lite Model)\n",
    "\n",
    "**Easy to provide:**\n",
    "- RAM (GB)\n",
    "- SSD (GB)\n",
    "- Brand\n",
    "- Screen size (inches)\n",
    "- Weight (kg)\n",
    "\n",
    "**Dropdown/selection:**\n",
    "- CPU brand & family\n",
    "- GPU brand & series\n",
    "- Screen technology (OLED, IPS, etc.)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Deploy models via API\n",
    "2. Create user interface with dropdowns for categorical features\n",
    "3. Implement CPU/GPU name → benchmark lookup for full model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}