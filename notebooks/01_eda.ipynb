{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - EDA: Computer Marketplace Dataset\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook performs an initial exploration of `db_computers_2025_raw.csv` - a dataset of computer listings from a marketplace.\n",
    "\n",
    "### Goals\n",
    "\n",
    "1. **Understand the target variable** (price) - its distribution, range, and potential transformations\n",
    "2. **Inspect distributions and missing values** across all columns\n",
    "3. **Identify core features** for the initial model:\n",
    "   - Product type (Tipo de producto)\n",
    "   - Brand (Marca) and Series (Serie)\n",
    "   - Processor (Procesador_Procesador)\n",
    "   - Graphics card (Gráfica_Tarjeta gráfica)\n",
    "   - RAM, storage, screen specs\n",
    "4. **Prepare observations** for the feature engineering notebook\n",
    "\n",
    "### Important Conventions\n",
    "\n",
    "- **Original Spanish column names are preserved** - we do NOT rename or translate columns\n",
    "- All comments and markdown are in English for readability\n",
    "- Engineered features (created in later notebooks) will start with underscore: `_cpu_mark`, `_ram_gb`, etc.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Display Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Seaborn style\n",
    "sns.set_theme(style='whitegrid', palette='deep')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths (relative to notebooks/ folder)\n",
    "DATA_DIR = Path('../data')\n",
    "\n",
    "# Main dataset\n",
    "computers_path = DATA_DIR / 'db_computers_2025_raw.csv'\n",
    "columns_info_path = DATA_DIR / 'db_computers_columns_names.txt'\n",
    "\n",
    "# Load the main dataset\n",
    "# Note: Using utf-8-sig to handle BOM, index_col=0 then reset_index per course instructions\n",
    "df = pd.read_csv(\n",
    "    computers_path,\n",
    "    encoding='utf-8-sig',\n",
    "    index_col=0\n",
    ").reset_index()\n",
    "\n",
    "# Rename the old index column if needed\n",
    "if 'index' in df.columns:\n",
    "    df = df.rename(columns={'index': 'id_original'})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"  - Rows (computer listings): {df.shape[0]:,}\")\n",
    "print(f\"  - Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first few rows\n",
    "# Note: Original Spanish column names are preserved exactly as they appear in the CSV\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all columns to see what's available\n",
    "print(\"All columns in the dataset:\\n\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:3d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Info and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic info about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values for key categorical columns\n",
    "# These are columns we suspect will be important for prediction\n",
    "\n",
    "key_columns = [\n",
    "    'Tipo de producto',           # Product type (laptop, desktop, etc.)\n",
    "    'Serie',                       # Product series/line\n",
    "    'Tipo',                        # Type (Laptop, etc.)\n",
    "    'Procesador_Fabricante del procesador',  # CPU manufacturer (Intel, AMD, Apple)\n",
    "    'Gráfica_Fabricante de la tarjeta gráfica',  # GPU manufacturer\n",
    "    'Sistema operativo_Sistema operativo',  # OS\n",
    "]\n",
    "\n",
    "print(\"Unique value counts for key categorical columns:\\n\")\n",
    "for col in key_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Unique values: {df[col].nunique()}\")\n",
    "        print(f\"  Missing: {df[col].isna().sum()} ({df[col].isna().mean()*100:.1f}%)\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for product type - this is crucial for understanding our dataset\n",
    "if 'Tipo de producto' in df.columns:\n",
    "    print(\"Product types (Tipo de producto):\\n\")\n",
    "    print(df['Tipo de producto'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for 'Tipo' column\n",
    "if 'Tipo' in df.columns:\n",
    "    print(\"Type (Tipo):\\n\")\n",
    "    print(df['Tipo'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU manufacturers\n",
    "if 'Procesador_Fabricante del procesador' in df.columns:\n",
    "    print(\"CPU Manufacturers (Procesador_Fabricante del procesador):\\n\")\n",
    "    print(df['Procesador_Fabricante del procesador'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Target Variable Exploration: Price (Precio)\n",
    "\n",
    "The price information is stored in the `Precio_Rango` column as a string range (e.g., \"1.026,53 € – 2.287,17 €\").\n",
    "\n",
    "For modeling, we'll need to extract a numeric value from this range. For now, let's explore it as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the price column\n",
    "precio_col = 'Precio_Rango'\n",
    "\n",
    "print(f\"Price column: {precio_col}\")\n",
    "print(f\"\\nSample values:\")\n",
    "print(df[precio_col].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_precio_numerico(precio_str):\n",
    "    \"\"\"\n",
    "    Extract numeric price from a range string.\n",
    "    For ranges like \"1.026,53 € – 2.287,17 €\", returns the midpoint.\n",
    "    For single prices, returns that price.\n",
    "    \n",
    "    Spanish format: periods for thousands, commas for decimals.\n",
    "    \"\"\"\n",
    "    if pd.isna(precio_str) or not isinstance(precio_str, str):\n",
    "        return np.nan\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    # Find all price patterns (number with Spanish format)\n",
    "    # Pattern: digits with optional periods, comma, digits\n",
    "    pattern = r'([\\d.]+,\\d{2})'\n",
    "    matches = re.findall(pattern, precio_str)\n",
    "    \n",
    "    if not matches:\n",
    "        return np.nan\n",
    "    \n",
    "    # Convert Spanish format to float\n",
    "    precios = []\n",
    "    for m in matches:\n",
    "        # Remove thousand separators (.), replace decimal comma with period\n",
    "        num_str = m.replace('.', '').replace(',', '.')\n",
    "        precios.append(float(num_str))\n",
    "    \n",
    "    # Return midpoint if range, otherwise single value\n",
    "    if len(precios) == 2:\n",
    "        return (precios[0] + precios[1]) / 2\n",
    "    elif len(precios) == 1:\n",
    "        return precios[0]\n",
    "    else:\n",
    "        return np.mean(precios)\n",
    "\n",
    "# Extract numeric prices (for EDA only - proper extraction in features.py)\n",
    "df['_precio_eda'] = df[precio_col].apply(extraer_precio_numerico)\n",
    "\n",
    "print(f\"Extracted numeric prices:\")\n",
    "print(f\"  Valid prices: {df['_precio_eda'].notna().sum()} / {len(df)}\")\n",
    "print(f\"  Missing: {df['_precio_eda'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price statistics\n",
    "print(\"Price Statistics (in €):\\n\")\n",
    "print(df['_precio_eda'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price distribution visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1 = axes[0]\n",
    "df['_precio_eda'].hist(bins=50, ax=ax1, edgecolor='black', alpha=0.7)\n",
    "ax1.set_xlabel('Price (€)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Price Distribution (Histogram)')\n",
    "ax1.axvline(df['_precio_eda'].median(), color='red', linestyle='--', label=f\"Median: {df['_precio_eda'].median():,.0f}€\")\n",
    "ax1.axvline(df['_precio_eda'].mean(), color='orange', linestyle='--', label=f\"Mean: {df['_precio_eda'].mean():,.0f}€\")\n",
    "ax1.legend()\n",
    "\n",
    "# Box plot\n",
    "ax2 = axes[1]\n",
    "df.boxplot(column='_precio_eda', ax=ax2)\n",
    "ax2.set_ylabel('Price (€)')\n",
    "ax2.set_title('Price Distribution (Box Plot)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Observations on price distribution\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- The distribution appears right-skewed (long tail of high prices)\")\n",
    "print(\"- There are potential outliers at the high end (expensive gaming/workstation PCs)\")\n",
    "print(\"- A log-transform might help normalize the distribution for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 4.2. Outlier Analysis\n\nOutliers in price data can significantly affect model training (especially RMSE). Let's identify and analyze potential outliers.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Outlier detection using multiple methods\nprint(\"=\" * 60)\nprint(\"OUTLIER ANALYSIS FOR PRICE\")\nprint(\"=\" * 60)\n\nprice_data = df['_precio_eda'].dropna()\n\n# Method 1: IQR (Interquartile Range)\nQ1 = price_data.quantile(0.25)\nQ3 = price_data.quantile(0.75)\nIQR = Q3 - Q1\nlower_bound_iqr = Q1 - 1.5 * IQR\nupper_bound_iqr = Q3 + 1.5 * IQR\n\noutliers_iqr = price_data[(price_data < lower_bound_iqr) | (price_data > upper_bound_iqr)]\n\nprint(f\"\\n1. IQR Method (1.5 * IQR):\")\nprint(f\"   Q1: €{Q1:,.0f}, Q3: €{Q3:,.0f}, IQR: €{IQR:,.0f}\")\nprint(f\"   Lower bound: €{lower_bound_iqr:,.0f}\")\nprint(f\"   Upper bound: €{upper_bound_iqr:,.0f}\")\nprint(f\"   Outliers detected: {len(outliers_iqr)} ({len(outliers_iqr)/len(price_data)*100:.1f}%)\")\n\n# Method 2: Percentile-based (1st and 99th percentile)\np1 = price_data.quantile(0.01)\np99 = price_data.quantile(0.99)\noutliers_percentile = price_data[(price_data < p1) | (price_data > p99)]\n\nprint(f\"\\n2. Percentile Method (1st-99th):\")\nprint(f\"   1st percentile: €{p1:,.0f}\")\nprint(f\"   99th percentile: €{p99:,.0f}\")\nprint(f\"   Outliers detected: {len(outliers_percentile)} ({len(outliers_percentile)/len(price_data)*100:.1f}%)\")\n\n# Method 3: Z-score (values > 3 std from mean)\nmean_price = price_data.mean()\nstd_price = price_data.std()\nz_threshold = 3\nlower_bound_z = mean_price - z_threshold * std_price\nupper_bound_z = mean_price + z_threshold * std_price\noutliers_zscore = price_data[(price_data < lower_bound_z) | (price_data > upper_bound_z)]\n\nprint(f\"\\n3. Z-Score Method (|z| > 3):\")\nprint(f\"   Mean: €{mean_price:,.0f}, Std: €{std_price:,.0f}\")\nprint(f\"   Lower bound: €{max(0, lower_bound_z):,.0f}\")\nprint(f\"   Upper bound: €{upper_bound_z:,.0f}\")\nprint(f\"   Outliers detected: {len(outliers_zscore)} ({len(outliers_zscore)/len(price_data)*100:.1f}%)\")\n\n# Show the extreme outliers\nprint(f\"\\n=== Most Extreme High Prices (Top 10) ===\")\nprint(price_data.nlargest(10).to_string())\n\nprint(f\"\\n=== Most Extreme Low Prices (Bottom 10) ===\")\nprint(price_data.nsmallest(10).to_string())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize outliers\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# 1. Distribution with outlier thresholds\nax1 = axes[0]\ndf['_precio_eda'].hist(bins=100, ax=ax1, edgecolor='black', alpha=0.7)\nax1.axvline(upper_bound_iqr, color='red', linestyle='--', lw=2, label=f'IQR upper: €{upper_bound_iqr:,.0f}')\nax1.axvline(p99, color='orange', linestyle='--', lw=2, label=f'99th pct: €{p99:,.0f}')\nax1.set_xlabel('Price (€)')\nax1.set_ylabel('Frequency')\nax1.set_title('Price Distribution with Outlier Thresholds')\nax1.legend()\n\n# 2. Box plot showing outliers\nax2 = axes[1]\nax2.boxplot(price_data, vert=True)\nax2.set_ylabel('Price (€)')\nax2.set_title('Box Plot (dots are outliers)')\n\n# 3. Distribution WITHOUT outliers (preview)\nax3 = axes[2]\nprice_no_outliers = price_data[(price_data >= p1) & (price_data <= p99)]\nprice_no_outliers.hist(bins=50, ax=ax3, edgecolor='black', alpha=0.7, color='green')\nax3.set_xlabel('Price (€)')\nax3.set_ylabel('Frequency')\nax3.set_title(f'Price Distribution WITHOUT Outliers\\n(1st-99th percentile, n={len(price_no_outliers):,})')\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics comparison\nprint(\"\\n=== Impact of Removing Outliers (1st-99th percentile) ===\")\nprint(f\"{'Metric':<15s} {'With Outliers':>15s} {'Without Outliers':>18s} {'Change':>12s}\")\nprint(\"-\" * 60)\nprint(f\"{'Count':<15s} {len(price_data):>15,} {len(price_no_outliers):>18,} {len(price_data)-len(price_no_outliers):>+12,}\")\nprint(f\"{'Mean':<15s} €{price_data.mean():>14,.0f} €{price_no_outliers.mean():>17,.0f} {(price_no_outliers.mean()-price_data.mean()):>+12,.0f}\")\nprint(f\"{'Std':<15s} €{price_data.std():>14,.0f} €{price_no_outliers.std():>17,.0f} {(price_no_outliers.std()-price_data.std()):>+12,.0f}\")\nprint(f\"{'Min':<15s} €{price_data.min():>14,.0f} €{price_no_outliers.min():>17,.0f}\")\nprint(f\"{'Max':<15s} €{price_data.max():>14,.0f} €{price_no_outliers.max():>17,.0f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Outlier Handling Recommendation\n\nBased on the analysis above:\n\n**Recommendation: Remove extreme outliers using the 1st-99th percentile method**\n\nReasons:\n1. **RMSE sensitivity**: RMSE squares errors, so extreme outliers disproportionately affect training\n2. **Data quality**: Very low prices (< €200) may be accessories or errors; very high prices (> €5000) are rare workstations\n3. **Model generalization**: Most users want predictions for \"typical\" computers, not extreme cases\n4. **Conservative approach**: 1st-99th percentile removes only ~2% of data while reducing noise\n\n**Implementation in `features.py`:**\n```python\n# Remove price outliers (1st-99th percentile)\np1 = df['_precio_num'].quantile(0.01)\np99 = df['_precio_num'].quantile(0.99)\ndf = df[(df['_precio_num'] >= p1) & (df['_precio_num'] <= p99)]\n```\n\n**Alternative**: For production, consider keeping outliers but using:\n- MAE instead of RMSE (less sensitive to outliers)\n- Quantile regression (robust to outliers)\n- Log-transform target (compresses extreme values)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transformed price distribution (preview for future consideration)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "df['_log_precio'] = np.log1p(df['_precio_eda'])\n",
    "df['_log_precio'].hist(bins=50, ax=ax, edgecolor='black', alpha=0.7, color='green')\n",
    "ax.set_xlabel('Log(Price + 1)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Log-Transformed Price Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Log-transform makes the distribution more symmetric.\")\n",
    "print(\"Consider using log-price as target, then exp() to get final predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Price Range Analysis: Range vs Midpoint\n",
    "\n",
    "The `Precio_Rango` column contains price ranges (min-max). We need to decide whether to:\n",
    "1. **Predict the midpoint** (simpler, single target)\n",
    "2. **Predict the range** (min and max separately, or range spread)\n",
    "\n",
    "Let's analyze the range spreads to inform this decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract min, max, and midpoint from price ranges\n",
    "import re\n",
    "\n",
    "def extract_full_price_range(precio_str):\n",
    "    \"\"\"Extract min, max, and midpoint from price range string.\"\"\"\n",
    "    if pd.isna(precio_str) or not isinstance(precio_str, str):\n",
    "        return None, None, None\n",
    "    \n",
    "    pattern = r'([\\d.]+,\\d{2})'\n",
    "    matches = re.findall(pattern, precio_str)\n",
    "    \n",
    "    if not matches:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Convert Spanish format to float\n",
    "    precios = []\n",
    "    for m in matches:\n",
    "        num_str = m.replace('.', '').replace(',', '.')\n",
    "        precios.append(float(num_str))\n",
    "    \n",
    "    if len(precios) == 2:\n",
    "        min_price = precios[0]\n",
    "        max_price = precios[1]\n",
    "        mid_price = (precios[0] + precios[1]) / 2\n",
    "        return min_price, max_price, mid_price\n",
    "    elif len(precios) == 1:\n",
    "        return precios[0], precios[0], precios[0]\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "# Apply extraction\n",
    "price_ranges = df['Precio_Rango'].apply(extract_full_price_range)\n",
    "df['_min_price'] = price_ranges.apply(lambda x: x[0])\n",
    "df['_max_price'] = price_ranges.apply(lambda x: x[1])\n",
    "df['_mid_price'] = price_ranges.apply(lambda x: x[2])\n",
    "\n",
    "# Calculate spread metrics\n",
    "df['_price_spread'] = df['_max_price'] - df['_min_price']\n",
    "df['_spread_pct'] = (df['_price_spread'] / df['_min_price'] * 100)\n",
    "\n",
    "print(\"Price Range Statistics:\\n\")\n",
    "print(df[['_min_price', '_max_price', '_mid_price', '_price_spread', '_spread_pct']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price spread distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Price spread in euros\n",
    "ax1 = axes[0]\n",
    "df['_price_spread'].hist(bins=50, ax=ax1, edgecolor='black', alpha=0.7, color='coral')\n",
    "ax1.set_xlabel('Price Spread (€)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Price Range Spread (Max - Min)')\n",
    "ax1.axvline(df['_price_spread'].median(), color='red', linestyle='--', \n",
    "            label=f\"Median: {df['_price_spread'].median():,.0f}€\")\n",
    "\n",
    "# Price spread as percentage\n",
    "ax2 = axes[1]\n",
    "df['_spread_pct'].hist(bins=50, ax=ax2, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "ax2.set_xlabel('Spread as % of Min Price')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Price Spread as Percentage of Min Price')\n",
    "ax2.axvline(df['_spread_pct'].median(), color='red', linestyle='--', \n",
    "            label=f\"Median: {df['_spread_pct'].median():.1f}%\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(f\"- Median price spread: {df['_price_spread'].median():,.0f}€\")\n",
    "print(f\"- Median spread as % of min price: {df['_spread_pct'].median():.1f}%\")\n",
    "print(f\"- Mean spread as % of min price: {df['_spread_pct'].mean():.1f}%\")\n",
    "print(\"\\n-> The price ranges are quite wide (median ~{:.0f}% of min price)\".format(df['_spread_pct'].median()))\n",
    "print(\"-> This suggests the marketplace aggregates offers from multiple sellers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Ofertas column - does it contain individual price listings?\n",
    "print(\"=== Ofertas Column Analysis ===\\n\")\n",
    "print(\"Sample values:\")\n",
    "print(df['Ofertas'].head(20))\n",
    "print(\"\\n\")\n",
    "print(f\"Unique values: {df['Ofertas'].nunique()}\")\n",
    "print(f\"Missing values: {df['Ofertas'].isna().sum()} ({df['Ofertas'].isna().mean()*100:.1f}%)\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Extract number of offers\n",
    "def extract_num_ofertas(oferta_str):\n",
    "    \"\"\"Extract number of offers from string like '200 ofertas:'\"\"\"\n",
    "    if pd.isna(oferta_str):\n",
    "        return np.nan\n",
    "    import re\n",
    "    match = re.search(r'(\\d+)\\s+ofertas?', str(oferta_str), re.IGNORECASE)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return np.nan\n",
    "\n",
    "df['_num_ofertas'] = df['Ofertas'].apply(extract_num_ofertas)\n",
    "\n",
    "print(\"Number of offers statistics:\")\n",
    "print(df['_num_ofertas'].describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Correlation between number of offers and price spread\n",
    "correlation = df[['_num_ofertas', '_price_spread', '_spread_pct']].corr()\n",
    "print(\"Correlation with price spread:\")\n",
    "print(correlation.loc['_num_ofertas'])\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Conclusion:\")\n",
    "print(\"- The 'Ofertas' column only contains the COUNT of offers (e.g., '200 ofertas:')\")\n",
    "print(\"- It does NOT contain individual price listings\")\n",
    "print(\"- The Precio_Rango represents the min-max across all those offers\")\n",
    "print(\"- More offers tends to correlate with wider price spreads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision: Predict Midpoint vs Range?\n",
    "\n",
    "Based on the analysis above, here's the recommendation:\n",
    "\n",
    "**Recommendation: Predict the MIDPOINT price** for the following reasons:\n",
    "\n",
    "1. **Simpler modeling approach**: Single target variable instead of two (min/max)\n",
    "2. **Marketplace context**: The price range represents offers from multiple sellers, not product uncertainty\n",
    "3. **User-friendly output**: A single price estimate is easier to interpret than a range\n",
    "4. **Wide spreads**: The ranges are wide (~95% median spread), so predicting the exact range is less meaningful\n",
    "\n",
    "**Alternative approaches** (for future consideration):\n",
    "- Predict both `min_price` and `max_price` separately (multi-target regression)\n",
    "- Predict `midpoint` + `spread` (could give users a confidence interval)\n",
    "- Build separate models for different price segments\n",
    "\n",
    "**Decision**: We'll use `_precio_num` (midpoint) as our target variable in `02_feature_engineering.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Missing Values Overview\n",
    "\n",
    "Understanding missing values is crucial for:\n",
    "- Deciding which features to include in the model\n",
    "- Choosing imputation strategies\n",
    "- Identifying potentially unreliable columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values per column\n",
    "missing_counts = df.isna().sum().sort_values(ascending=False)\n",
    "missing_pct = (df.isna().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "\n",
    "# Create a summary DataFrame\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Missing %': missing_pct,\n",
    "    'Present %': 100 - missing_pct\n",
    "})\n",
    "\n",
    "# Show columns with most missing values\n",
    "print(\"Columns with MOST missing values (top 30):\\n\")\n",
    "print(missing_df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show columns with LEAST missing values (most complete)\n",
    "print(\"Columns with LEAST missing values (most complete):\\n\")\n",
    "print(missing_df.tail(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values for key feature columns\n",
    "# Focus on columns we expect to use for modeling\n",
    "\n",
    "key_feature_cols = [\n",
    "    'Precio_Rango',\n",
    "    'Título',\n",
    "    'Tipo de producto',\n",
    "    'Serie',\n",
    "    'Procesador_Procesador',\n",
    "    'Procesador_Fabricante del procesador',\n",
    "    'Procesador_Número de núcleos del procesador',\n",
    "    'RAM_Memoria RAM',\n",
    "    'RAM_Tipo de RAM',\n",
    "    'Disco duro_Capacidad de memoria SSD',\n",
    "    'Disco duro_Tipo de disco duro',\n",
    "    'Gráfica_Tarjeta gráfica',\n",
    "    'Gráfica_Fabricante de la tarjeta gráfica',\n",
    "    'Gráfica_Memoria gráfica',\n",
    "    'Pantalla_Diagonal de la pantalla',\n",
    "    'Pantalla_Resolución de pantalla',\n",
    "    'Tipo',\n",
    "]\n",
    "\n",
    "# Filter to columns that exist\n",
    "key_feature_cols = [c for c in key_feature_cols if c in df.columns]\n",
    "\n",
    "key_missing = missing_df.loc[key_feature_cols].sort_values('Missing %', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "key_missing['Present %'].plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_xlabel('Data Completeness (%)')\n",
    "ax.set_title('Data Completeness for Key Feature Columns')\n",
    "ax.set_xlim(0, 100)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(key_missing['Present %']):\n",
    "    ax.text(v + 1, i, f'{v:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize columns by missing data levels\n",
    "very_sparse = missing_df[missing_df['Missing %'] >= 80].index.tolist()  # >80% missing\n",
    "sparse = missing_df[(missing_df['Missing %'] >= 50) & (missing_df['Missing %'] < 80)].index.tolist()\n",
    "moderate = missing_df[(missing_df['Missing %'] >= 20) & (missing_df['Missing %'] < 50)].index.tolist()\n",
    "dense = missing_df[missing_df['Missing %'] < 20].index.tolist()  # <20% missing\n",
    "\n",
    "print(f\"Very Sparse (>80% missing): {len(very_sparse)} columns\")\n",
    "print(f\"Sparse (50-80% missing): {len(sparse)} columns\")\n",
    "print(f\"Moderate (20-50% missing): {len(moderate)} columns\")\n",
    "print(f\"Dense (<20% missing): {len(dense)} columns\")\n",
    "\n",
    "print(\"\\n--- Dense columns (likely useful for initial model) ---\")\n",
    "for col in dense:\n",
    "    print(f\"  {col}: {missing_df.loc[col, 'Present %']:.1f}% present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values Observations\n",
    "\n",
    "**Dense columns (low missingness, likely useful):**\n",
    "- `Título` (Title) - Always present, can extract brand info\n",
    "- `Precio_Rango` - Target variable, mostly present\n",
    "- `Tipo de producto` - Product category (gaming, multimedia, etc.)\n",
    "- `Tipo` - Laptop vs Desktop classification\n",
    "\n",
    "**Moderate missingness (need imputation):**\n",
    "- `Procesador_Procesador` - CPU name, will need fuzzy matching to benchmarks\n",
    "- `RAM_Memoria RAM` - RAM info, extractable with regex\n",
    "- Screen-related columns\n",
    "\n",
    "**Sparse columns (might drop or use carefully):**\n",
    "- Many detailed spec columns are very sparse\n",
    "- Some columns only apply to specific product types (e.g., battery for laptops only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. First Thoughts on Core Features\n",
    "\n",
    "Based on domain knowledge about computer pricing, here are the features we expect to be most predictive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core features we plan to use for the initial model\n",
    "# Note: We'll create engineered versions of many of these with _ prefix\n",
    "\n",
    "posibles_features_core = [\n",
    "    # Category/Type\n",
    "    'Tipo de producto',           # Product type (gaming, multimedia, professional)\n",
    "    'Tipo',                        # Laptop vs Desktop\n",
    "    'Serie',                       # Product series (often indicates tier)\n",
    "    \n",
    "    # Processor\n",
    "    'Procesador_Procesador',       # CPU model - will map to _cpu_mark benchmark\n",
    "    'Procesador_Fabricante del procesador',  # Intel, AMD, Apple\n",
    "    'Procesador_Número de núcleos del procesador',  # Core count\n",
    "    \n",
    "    # Graphics\n",
    "    'Gráfica_Tarjeta gráfica',     # GPU model - will map to _gpu_mark benchmark\n",
    "    'Gráfica_Fabricante de la tarjeta gráfica',  # NVIDIA, AMD, Intel\n",
    "    'Gráfica_Memoria gráfica',     # VRAM\n",
    "    \n",
    "    # Memory and Storage\n",
    "    'RAM_Memoria RAM',             # RAM - will extract to _ram_gb\n",
    "    'RAM_Tipo de RAM',             # DDR4, DDR5, LPDDR5X\n",
    "    'Disco duro_Capacidad de memoria SSD',  # SSD - will extract to _ssd_gb\n",
    "    'Disco duro_Tipo de disco duro',  # SSD, HDD, hybrid\n",
    "    \n",
    "    # Display\n",
    "    'Pantalla_Diagonal de la pantalla',  # Screen size - will extract to _tamano_pantalla_pulgadas\n",
    "    'Pantalla_Resolución de pantalla',   # Resolution\n",
    "    'Pantalla_Tasa de actualización de imagen',  # Refresh rate (important for gaming)\n",
    "]\n",
    "\n",
    "print(\"Core features for initial model:\")\n",
    "for i, feat in enumerate(posibles_features_core, 1):\n",
    "    if feat in df.columns:\n",
    "        present_pct = (1 - df[feat].isna().mean()) * 100\n",
    "        print(f\"{i:2d}. {feat}: {present_pct:.1f}% present\")\n",
    "    else:\n",
    "        print(f\"{i:2d}. {feat}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Brand Extraction from Título\n",
    "\n",
    "The dataset may not have a `Marca` (brand) column, but we can extract it from the `Título` column.\n",
    "Let's explore how to identify brands from product titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Marca column exists\n",
    "if 'Marca' in df.columns:\n",
    "    print(\"Marca column EXISTS\\n\")\n",
    "    print(df['Marca'].value_counts(dropna=False).head(20))\n",
    "    print(f\"\\nMissing: {df['Marca'].isna().sum()} ({df['Marca'].isna().mean()*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"Marca column DOES NOT EXIST - we need to extract it from Título\\n\")\n",
    "    \n",
    "# Sample titles to understand the pattern\n",
    "print(\"\\n=== Sample Títulos ===\")\n",
    "print(df['Título'].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract brand from title (usually the first word)\n",
    "def extract_brand(titulo):\n",
    "    \"\"\"Extract brand from product title (usually first word).\"\"\"\n",
    "    if pd.isna(titulo):\n",
    "        return np.nan\n",
    "    \n",
    "    # Common computer brands (case insensitive)\n",
    "    common_brands = [\n",
    "        'Apple', 'ASUS', 'Lenovo', 'HP', 'Dell', 'Acer', 'MSI', \n",
    "        'Samsung', 'Microsoft', 'Razer', 'Alienware', 'LG', \n",
    "        'Huawei', 'Xiaomi', 'GigaByte', 'Gigabyte', 'Toshiba',\n",
    "        'Fujitsu', 'Medion', 'Sony', 'Vaio', 'Corsair', 'NZXT'\n",
    "    ]\n",
    "    \n",
    "    # Get first word (usually the brand)\n",
    "    first_word = str(titulo).split()[0] if titulo else ''\n",
    "    \n",
    "    # Check if first word matches a known brand (case insensitive)\n",
    "    for brand in common_brands:\n",
    "        if first_word.lower() == brand.lower():\n",
    "            return brand\n",
    "    \n",
    "    # If not found in common brands, return the first word anyway\n",
    "    # (we'll clean this up in feature engineering)\n",
    "    return first_word if first_word else np.nan\n",
    "\n",
    "df['_brand_extracted'] = df['Título'].apply(extract_brand)\n",
    "\n",
    "print(\"Extracted Brand Distribution:\\n\")\n",
    "print(df['_brand_extracted'].value_counts(dropna=False).head(20))\n",
    "print(f\"\\nUnique brands: {df['_brand_extracted'].nunique()}\")\n",
    "print(f\"Missing: {df['_brand_extracted'].isna().sum()}\")\n",
    "\n",
    "# Compare with Serie column to see if brand info is there\n",
    "print(\"\\n=== Serie Column (for comparison) ===\")\n",
    "if 'Serie' in df.columns:\n",
    "    print(df['Serie'].value_counts(dropna=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Serie Missing Value Imputation from Título\n",
    "\n",
    "The `Serie` column has many missing values. Let's explore if we can infer the Series from the product title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Serie column\n",
    "print(\"=== Serie Column Analysis ===\\n\")\n",
    "if 'Serie' in df.columns:\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"Serie present: {df['Serie'].notna().sum()} ({df['Serie'].notna().mean()*100:.1f}%)\")\n",
    "    print(f\"Serie missing: {df['Serie'].isna().sum()} ({df['Serie'].isna().mean()*100:.1f}%)\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Look at some examples where Serie is present\n",
    "    print(\"Examples with Serie present:\")\n",
    "    sample_with_serie = df[df['Serie'].notna()][['Título', 'Serie', '_brand_extracted']].head(15)\n",
    "    print(sample_with_serie.to_string())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Look at some examples where Serie is missing\n",
    "    print(\"Examples with Serie MISSING:\")\n",
    "    sample_without_serie = df[df['Serie'].isna()][['Título', 'Serie', '_brand_extracted']].head(15)\n",
    "    print(sample_without_serie.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract potential series from title\n",
    "def extract_series_from_title(titulo, brand):\n",
    "    \"\"\"\n",
    "    Attempt to extract product series from title.\n",
    "    Common patterns:\n",
    "    - Apple: MacBook Air, MacBook Pro, iMac\n",
    "    - ASUS: ROG, Zenbook, Vivobook, TUF Gaming\n",
    "    - Lenovo: ThinkPad, IdeaPad, Legion, LOQ\n",
    "    - HP: Pavilion, Envy, Omen, EliteBook\n",
    "    - Dell: Inspiron, XPS, Alienware, Latitude\n",
    "    \"\"\"\n",
    "    if pd.isna(titulo):\n",
    "        return np.nan\n",
    "    \n",
    "    titulo_lower = str(titulo).lower()\n",
    "    \n",
    "    # Known series patterns by brand\n",
    "    series_patterns = {\n",
    "        'Apple': ['MacBook Air', 'MacBook Pro', 'iMac', 'Mac Mini', 'Mac Pro', 'Mac Studio'],\n",
    "        'ASUS': ['ROG', 'Zenbook', 'Vivobook', 'TUF Gaming', 'Republic of Gamers', \n",
    "                 'ExpertBook', 'ProArt', 'StudioBook', 'Chromebook'],\n",
    "        'Lenovo': ['ThinkPad', 'IdeaPad', 'Legion', 'LOQ', 'Yoga', 'ThinkBook'],\n",
    "        'HP': ['Pavilion', 'Envy', 'Omen', 'EliteBook', 'ProBook', 'Spectre', 'ZBook'],\n",
    "        'Dell': ['Inspiron', 'XPS', 'Alienware', 'Latitude', 'Precision', 'Vostro'],\n",
    "        'MSI': ['Katana', 'Stealth', 'Raider', 'Cyborg', 'Prestige', 'Modern', 'Summit'],\n",
    "        'Acer': ['Aspire', 'Swift', 'Nitro', 'Predator', 'TravelMate', 'ConceptD'],\n",
    "        'Samsung': ['Galaxy Book'],\n",
    "        'Microsoft': ['Surface'],\n",
    "        'Gigabyte': ['Aero', 'Aorus'],\n",
    "    }\n",
    "    \n",
    "    if pd.isna(brand) or brand not in series_patterns:\n",
    "        return np.nan\n",
    "    \n",
    "    # Look for series keywords in title\n",
    "    for series in series_patterns[brand]:\n",
    "        if series.lower() in titulo_lower:\n",
    "            return series\n",
    "    \n",
    "    return np.nan\n",
    "\n",
    "df['_series_extracted'] = df.apply(\n",
    "    lambda row: extract_series_from_title(row['Título'], row['_brand_extracted']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"=== Series Extraction Results ===\\n\")\n",
    "print(f\"Series extracted: {df['_series_extracted'].notna().sum()}\")\n",
    "print(f\"Could not extract: {df['_series_extracted'].isna().sum()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Compare extracted series with existing Serie column\n",
    "if 'Serie' in df.columns:\n",
    "    print(\"Comparison: Existing Serie vs Extracted Series\")\n",
    "    comparison = pd.DataFrame({\n",
    "        'Serie_exists': df['Serie'].notna(),\n",
    "        'Serie_extracted': df['_series_extracted'].notna()\n",
    "    })\n",
    "    \n",
    "    print(\"\\nCrosstab:\")\n",
    "    print(pd.crosstab(comparison['Serie_exists'], comparison['Serie_extracted'], \n",
    "                      rownames=['Serie column present'], \n",
    "                      colnames=['Extracted from title']))\n",
    "    \n",
    "    # Show some examples where we filled missing Serie\n",
    "    print(\"\\n=== Examples where we filled missing Serie ===\")\n",
    "    filled = df[(df['Serie'].isna()) & (df['_series_extracted'].notna())]\n",
    "    if len(filled) > 0:\n",
    "        print(filled[['Título', 'Serie', '_series_extracted', '_brand_extracted']].head(10).to_string())\n",
    "        print(f\"\\nTotal filled: {len(filled)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Screen Size: Pulgadas vs Centimeters\n",
    "\n",
    "There are two screen size columns:\n",
    "- `Pantalla_Tamaño de la pantalla` - in pulgadas (inches) like \"15,6 pulgadas\"\n",
    "- `Pantalla_Diagonal de la pantalla` - in centimeters like \"39,624 cm\"\n",
    "\n",
    "Let's check which one is more complete and if we need both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both screen size columns\n",
    "screen_cols = {\n",
    "    'Pantalla_Tamaño de la pantalla': 'pulgadas (inches)',\n",
    "    'Pantalla_Diagonal de la pantalla': 'cm'\n",
    "}\n",
    "\n",
    "print(\"=== Screen Size Column Comparison ===\\n\")\n",
    "for col, unit in screen_cols.items():\n",
    "    if col in df.columns:\n",
    "        present_count = df[col].notna().sum()\n",
    "        present_pct = df[col].notna().mean() * 100\n",
    "        print(f\"{col} ({unit}):\")\n",
    "        print(f\"  Present: {present_count} ({present_pct:.1f}%)\")\n",
    "        print(f\"  Missing: {df[col].isna().sum()} ({df[col].isna().mean()*100:.1f}%)\")\n",
    "        print(f\"  Sample values: {df[col].dropna().head(5).tolist()}\")\n",
    "        print()\n",
    "\n",
    "# Check if they're redundant (one can be converted from the other)\n",
    "if 'Pantalla_Tamaño de la pantalla' in df.columns and 'Pantalla_Diagonal de la pantalla' in df.columns:\n",
    "    print(\"\\n=== Checking correlation between pulgadas and cm ===\")\n",
    "    \n",
    "    # Extract numeric values from both\n",
    "    def extract_pulgadas(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        import re\n",
    "        # Match patterns like \"15,6 pulgadas\" or \"15.6 pulgadas\"\n",
    "        match = re.search(r'([\\d,\\.]+)', str(val))\n",
    "        if match:\n",
    "            num_str = match.group(1).replace(',', '.')\n",
    "            return float(num_str)\n",
    "        return np.nan\n",
    "    \n",
    "    def extract_cm(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        import re\n",
    "        # Match patterns like \"39,624 cm\"\n",
    "        match = re.search(r'([\\d,\\.]+)', str(val))\n",
    "        if match:\n",
    "            num_str = match.group(1).replace(',', '.')\n",
    "            return float(num_str)\n",
    "        return np.nan\n",
    "    \n",
    "    df['_pulgadas_num'] = df['Pantalla_Tamaño de la pantalla'].apply(extract_pulgadas)\n",
    "    df['_cm_num'] = df['Pantalla_Diagonal de la pantalla'].apply(extract_cm)\n",
    "    \n",
    "    # Check correlation (1 inch = 2.54 cm)\n",
    "    df['_cm_from_pulgadas'] = df['_pulgadas_num'] * 2.54\n",
    "    \n",
    "    # Compare for rows where both are present\n",
    "    both_present = df[(df['_pulgadas_num'].notna()) & (df['_cm_num'].notna())]\n",
    "    \n",
    "    if len(both_present) > 0:\n",
    "        print(f\"\\nRows with both values: {len(both_present)}\")\n",
    "        print(\"\\nSample comparison:\")\n",
    "        sample = both_present[['Pantalla_Tamaño de la pantalla', 'Pantalla_Diagonal de la pantalla', \n",
    "                               '_pulgadas_num', '_cm_num', '_cm_from_pulgadas']].head(10)\n",
    "        print(sample.to_string())\n",
    "        \n",
    "        # Check if they're consistent (allowing for rounding)\n",
    "        diff = np.abs(both_present['_cm_num'] - both_present['_cm_from_pulgadas'])\n",
    "        print(f\"\\nDifference between cm column and converted pulgadas:\")\n",
    "        print(f\"  Mean diff: {diff.mean():.3f} cm\")\n",
    "        print(f\"  Max diff: {diff.max():.3f} cm\")\n",
    "        \n",
    "        consistent = (diff < 0.1).sum()\n",
    "        print(f\"\\n  Consistent (diff < 0.1 cm): {consistent}/{len(both_present)} ({consistent/len(both_present)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n=== Coverage Analysis ===\")\n",
    "    coverage = pd.DataFrame({\n",
    "        'Has pulgadas': df['_pulgadas_num'].notna(),\n",
    "        'Has cm': df['_cm_num'].notna()\n",
    "    })\n",
    "    print(pd.crosstab(coverage['Has pulgadas'], coverage['Has cm'], \n",
    "                      rownames=['Has pulgadas'], colnames=['Has cm'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screen Size Recommendation\n",
    "\n",
    "Based on the analysis above:\n",
    "\n",
    "**Recommendation: Use `Pantalla_Tamaño de la pantalla` (pulgadas/inches) as the primary feature**\n",
    "\n",
    "Reasons:\n",
    "1. **Industry standard**: Screen sizes are typically marketed in inches (13\", 15.6\", 17\", etc.)\n",
    "2. **More intuitive**: Users and domain experts think in inches for laptops/monitors\n",
    "3. **Likely better coverage**: The pulgadas column probably has similar or better coverage than cm\n",
    "4. **Redundancy**: If both columns are present and consistent, we only need one\n",
    "5. **Feature engineering**: We can convert cm to inches if pulgadas is missing\n",
    "\n",
    "**Strategy for feature engineering** ([02_feature_engineering.ipynb](notebooks/02_feature_engineering.ipynb)):\n",
    "- Extract numeric value from `Pantalla_Tamaño de la pantalla` to create `_tamano_pantalla_pulgadas`\n",
    "- If missing, convert `Pantalla_Diagonal de la pantalla` (cm ÷ 2.54 = inches)\n",
    "- This gives us maximum coverage with a single, interpretable feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore RAM column\n",
    "print(\"RAM_Memoria RAM - Value distribution:\\n\")\n",
    "if 'RAM_Memoria RAM' in df.columns:\n",
    "    print(df['RAM_Memoria RAM'].value_counts(dropna=False).head(20))\n",
    "    print(\"\\n-> RAM seems mostly present, values like '16 GB RAM', '8 GB RAM', etc.\")\n",
    "    print(\"-> Will need to extract numeric value to _ram_gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore processor column\n",
    "print(\"Procesador_Procesador - Sample values:\\n\")\n",
    "if 'Procesador_Procesador' in df.columns:\n",
    "    print(df['Procesador_Procesador'].value_counts(dropna=False).head(20))\n",
    "    print(\"\\n-> Many unique CPU models (Intel Core i7-13700H, AMD Ryzen 7, Apple M3, etc.)\")\n",
    "    print(\"-> Will need to map to benchmark scores using db_cpu_raw.csv and fuzzy matching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore GPU column\n",
    "print(\"Gráfica_Tarjeta gráfica - Sample values:\\n\")\n",
    "if 'Gráfica_Tarjeta gráfica' in df.columns:\n",
    "    print(df['Gráfica_Tarjeta gráfica'].value_counts(dropna=False).head(20))\n",
    "    print(\"\\n-> Some GPU fields are sparse (integrated graphics often not listed)\")\n",
    "    print(\"-> Will need fuzzy matching to db_gpu_raw.csv for _gpu_mark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore SSD capacity column\n",
    "print(\"Disco duro_Capacidad de memoria SSD - Sample values:\\n\")\n",
    "if 'Disco duro_Capacidad de memoria SSD' in df.columns:\n",
    "    print(df['Disco duro_Capacidad de memoria SSD'].value_counts(dropna=False).head(20))\n",
    "    print(\"\\n-> Values like '512 GB', '1.000 GB', '256 GB'\")\n",
    "    print(\"-> Will extract to numeric _ssd_gb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore screen size column\n",
    "print(\"Pantalla_Diagonal de la pantalla - Sample values:\\n\")\n",
    "if 'Pantalla_Diagonal de la pantalla' in df.columns:\n",
    "    print(df['Pantalla_Diagonal de la pantalla'].value_counts(dropna=False).head(15))\n",
    "    print(\"\\n-> Values in cm like '39,624 cm', '35,56 cm'\")\n",
    "    print(\"-> Will convert to inches for _tamano_pantalla_pulgadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price by product type - initial exploration\n",
    "if 'Tipo de producto' in df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Filter to types with enough samples\n",
    "    tipo_counts = df['Tipo de producto'].value_counts()\n",
    "    main_tipos = tipo_counts[tipo_counts >= 10].index\n",
    "    \n",
    "    df_plot = df[df['Tipo de producto'].isin(main_tipos)]\n",
    "    \n",
    "    df_plot.boxplot(column='_precio_eda', by='Tipo de producto', ax=ax)\n",
    "    ax.set_xlabel('Tipo de producto')\n",
    "    ax.set_ylabel('Price (€)')\n",
    "    ax.set_title('Price Distribution by Product Type')\n",
    "    plt.suptitle('')  # Remove automatic title\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nObservation: Gaming laptops (Portátil gaming) tend to have higher prices.\")\n",
    "    print(\"Product type will be an important categorical feature.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis: Numerical Features vs Price\n",
    "\n",
    "Extract all numerical features and analyze their correlation with price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract numerical features for correlation analysis\nimport re\n\n# Extract RAM (GB)\ndef extract_numeric_gb(text):\n    \"\"\"Extract numeric value with GB/TB units.\"\"\"\n    if pd.isna(text):\n        return np.nan\n    text = str(text).upper().replace('.', '')  # Remove thousand separators\n    # Handle TB\n    match_tb = re.search(r'(\\d+)\\s*TB', text)\n    if match_tb:\n        return float(match_tb.group(1)) * 1000\n    # Handle GB\n    match_gb = re.search(r'(\\d+)\\s*GB', text)\n    if match_gb:\n        return float(match_gb.group(1))\n    return np.nan\n\n# Extract general numeric values (improved)\ndef extract_numeric(text):\n    \"\"\"Extract first numeric value from text.\"\"\"\n    if pd.isna(text):\n        return np.nan\n    # Look for integers or decimals (with comma or period as decimal separator)\n    match = re.search(r'(\\d+(?:[.,]\\d+)?)', str(text))\n    if match:\n        num_str = match.group(1).replace(',', '.')\n        try:\n            return float(num_str)\n        except ValueError:\n            return np.nan\n    return np.nan\n\n# Extract resolution components\ndef extract_resolution(res_str):\n    \"\"\"Extract width and height from resolution.\"\"\"\n    if pd.isna(res_str):\n        return np.nan, np.nan\n    match = re.search(r'(\\d+)\\s*[xX×]\\s*(\\d+)', str(res_str))\n    if match:\n        return float(match.group(1)), float(match.group(2))\n    return np.nan, np.nan\n\n# Feature extraction\ndf['_ram_gb'] = df['RAM_Memoria RAM'].apply(extract_numeric_gb) if 'RAM_Memoria RAM' in df.columns else np.nan\ndf['_ssd_gb'] = df['Disco duro_Capacidad de memoria SSD'].apply(extract_numeric_gb) if 'Disco duro_Capacidad de memoria SSD' in df.columns else np.nan\ndf['_screen_inches'] = df['Pantalla_Tamaño de la pantalla'].apply(extract_numeric) if 'Pantalla_Tamaño de la pantalla' in df.columns else np.nan\ndf['_cpu_cores'] = df['Procesador_Número de núcleos del procesador'].apply(extract_numeric) if 'Procesador_Número de núcleos del procesador' in df.columns else np.nan\ndf['_gpu_memory_gb'] = df['Gráfica_Memoria gráfica'].apply(extract_numeric_gb) if 'Gráfica_Memoria gráfica' in df.columns else np.nan\n\n# Resolution\nif 'Pantalla_Resolución de pantalla' in df.columns:\n    res_data = df['Pantalla_Resolución de pantalla'].apply(extract_resolution)\n    df['_res_width'] = res_data.apply(lambda x: x[0])\n    df['_res_height'] = res_data.apply(lambda x: x[1])\n    df['_total_pixels'] = df['_res_width'] * df['_res_height'] / 1_000_000  # In millions\nelse:\n    df['_res_width'] = np.nan\n    df['_res_height'] = np.nan\n    df['_total_pixels'] = np.nan\n\n# Refresh rate\nif 'Pantalla_Tasa de actualización de imagen' in df.columns:\n    df['_refresh_rate'] = df['Pantalla_Tasa de actualización de imagen'].apply(extract_numeric)\nelse:\n    df['_refresh_rate'] = np.nan\n\n# Weight (convert to kg)\nif 'Dimensiones_Peso' in df.columns:\n    def extract_weight_kg(text):\n        if pd.isna(text):\n            return np.nan\n        text = str(text).replace(',', '.')\n        match_kg = re.search(r'([\\d\\.]+)\\s*kg', text, re.IGNORECASE)\n        if match_kg:\n            try:\n                return float(match_kg.group(1))\n            except ValueError:\n                return np.nan\n        match_g = re.search(r'([\\d\\.]+)\\s*g', text, re.IGNORECASE)\n        if match_g:\n            try:\n                return float(match_g.group(1)) / 1000\n            except ValueError:\n                return np.nan\n        return np.nan\n    df['_weight_kg'] = df['Dimensiones_Peso'].apply(extract_weight_kg)\nelse:\n    df['_weight_kg'] = np.nan\n\nprint(\"Extracted numerical features:\")\nfeatures = ['_ram_gb', '_ssd_gb', '_screen_inches', '_cpu_cores', '_gpu_memory_gb', \n            '_total_pixels', '_refresh_rate', '_weight_kg', '_num_ofertas']\nfor feat in features:\n    if feat in df.columns:\n        valid = df[feat].notna().sum()\n        pct = (valid / len(df)) * 100\n        print(f\"  {feat:20s}: {valid:5,} values ({pct:5.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations with mid price\n",
    "features_to_correlate = ['_ram_gb', '_ssd_gb', '_screen_inches', '_cpu_cores', \n",
    "                         '_gpu_memory_gb', '_total_pixels', '_refresh_rate', \n",
    "                         '_weight_kg', '_num_ofertas']\n",
    "\n",
    "# Filter to available features\n",
    "available_features = [f for f in features_to_correlate if f in df.columns and df[f].notna().sum() > 10]\n",
    "\n",
    "# Compute correlations\n",
    "correlations = df[available_features + ['_mid_price']].corr()['_mid_price'].drop('_mid_price').sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n=== Correlations with Price ===\\n\")\n",
    "print(f\"{'Feature':<20s} {'Correlation':>12s} {'Strength':>15s}\")\n",
    "print(\"-\" * 50)\n",
    "for feat, corr in correlations.items():\n",
    "    if abs(corr) >= 0.5:\n",
    "        strength = \"Very Strong\"\n",
    "    elif abs(corr) >= 0.3:\n",
    "        strength = \"Moderate\"\n",
    "    else:\n",
    "        strength = \"Weak\"\n",
    "    print(f\"{feat:<20s} {corr:>12.3f} {strength:>15s}\")\n",
    "\n",
    "# Visualization: Bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "correlations.plot(kind='barh', ax=ax, color=['green' if x > 0 else 'red' for x in correlations])\n",
    "ax.set_xlabel('Correlation with Price', fontsize=12)\n",
    "ax.set_title('Feature Correlations with Mid Price', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0.3, color='orange', linestyle='--', alpha=0.5, label='Moderate (0.3)')\n",
    "ax.axvline(x=-0.3, color='orange', linestyle='--', alpha=0.5)\n",
    "ax.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='Strong (0.5)')\n",
    "ax.axvline(x=-0.5, color='red', linestyle='--', alpha=0.5)\n",
    "ax.legend()\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights:**\n",
    "- Strong correlations (|r| > 0.5) indicate features highly predictive of price\n",
    "- Moderate correlations (0.3 < |r| < 0.5) are useful but less dominant\n",
    "- Weak correlations (|r| < 0.3) may still be useful in combination with other features\n",
    "- Negative correlations mean the feature inversely relates to price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "The EDA revealed key insights about the computer marketplace dataset and informed our modeling strategy.\n",
    "\n",
    "**Next:** Feature engineering in [02_feature_engineering.ipynb](02_feature_engineering.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary columns\n",
    "temp_cols = ['_precio_eda', '_log_precio', '_min_price', '_max_price', '_mid_price', \n",
    "             '_price_spread', '_spread_pct', '_num_ofertas', '_brand_extracted', \n",
    "             '_series_extracted', '_pulgadas_num', '_cm_num', '_cm_from_pulgadas',\n",
    "             '_ram_gb', '_ssd_gb', '_screen_inches', '_cpu_cores', '_gpu_memory_gb',\n",
    "             '_res_width', '_res_height', '_total_pixels', '_refresh_rate', '_weight_kg']\n",
    "\n",
    "dropped = 0\n",
    "for col in temp_cols:\n",
    "    if col in df.columns:\n",
    "        df.drop(columns=[col], inplace=True)\n",
    "        dropped += 1\n",
    "\n",
    "print(f\"EDA complete! Dropped {dropped} temporary columns.\")\n",
    "print(\"Next: 02_feature_engineering.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}